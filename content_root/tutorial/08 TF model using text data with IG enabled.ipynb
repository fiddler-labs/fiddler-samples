{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Tutorial for Text Data with Integrated Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Fiddler Client\n",
    "We begin this section as usual by establishing a connection to our\n",
    "Fiddler instance. We can establish this connection either by specifying \n",
    "our credentials directly, or by utilizing our `fiddler.ini` file. More\n",
    "information can be found in the [setup](https://github.com/fiddler-labs/fiddler-samples/blob/master/content_root/tutorial/00%20Setup.ipynb) section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiddler as fdl\n",
    "\n",
    "# client = fdl.FiddlerApi(url=url, org_id=org_id, auth_token=auth_token)\n",
    "client = fdl.FiddlerApi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "Here we will load in our baseline dataset from a csv called `imdb_rnn.csv`. We will\n",
    "also create a schema using this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/app/fiddler_samples/samples/datasets/imdb_rnn/imdb_rnn.csv')\n",
    "df_schema = fdl.DatasetInfo.from_dataframe(df, max_inferred_cardinality=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Dataset\n",
    "To upload a model, you first need to upload a sample of the data of the model’s \n",
    "inputs, targets, and additional metadata that might be useful for model analysis. \n",
    "This data sample helps us (among other things) to infer the model schema and the \n",
    "data types and values range of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'imdb_rnn' not in client.list_datasets():\n",
    "    upload_result = client.upload_dataset(\n",
    "        dataset={'train': df}, \n",
    "        dataset_id='imdb_rnn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model Schema\n",
    "As you must have noted, in the dataset upload step we did not ask for the model’s \n",
    "features and targets, or any model specific information. That’s because we \n",
    "allow for linking multiple models to a given dataset schema. Hence we require \n",
    "an Infer model schema step which helps us know the features relevant to the \n",
    "model and the model task. Here you can specify the input features, the target \n",
    "column, decision columns and metadata columns, and also the type of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'polarity'\n",
    "feature_columns = ['sentence']\n",
    "train_input = df[feature_columns]\n",
    "train_target = df[target]\n",
    "\n",
    "model_info = fdl.ModelInfo.from_dataset_info(\n",
    "    dataset_info=client.get_dataset_info('imdb_rnn'),\n",
    "    target=target, \n",
    "    features=feature_columns,\n",
    "    display_name='Text IG',\n",
    "    description='this is a tensorflow model using text data and IG enabled from tutorial',\n",
    "    input_type=fdl.ModelInputType.TEXT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Tensorflow if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the following line if you need to install Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow==1.14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "Build and train your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "train_target = le.fit_transform(train_target)\n",
    "train_target = train_target.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "vocab_size = 1000\n",
    "max_seq_length = 150\n",
    "tok = Tokenizer(num_words=vocab_size)\n",
    "tok.fit_on_texts(train_input['sentence'])\n",
    "sequences = tok.texts_to_sequences(train_input['sentence'])\n",
    "sequences_matrix = sequence.pad_sequences(sequences, maxlen=max_seq_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def RNN():\n",
    "    inputs = Input(name='inputs', shape=[max_seq_length])\n",
    "    layer = Embedding(vocab_size, 64, input_length=max_seq_length)(inputs)\n",
    "    layer = LSTM(64)(layer)\n",
    "    layer = Dense(256, name='FC1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.2)(layer)\n",
    "    layer = Dense(1, name='out_layer')(layer)\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "    model = Model(inputs=inputs, outputs=layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy', optimizer=RMSprop(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "model.fit(sequences_matrix, train_target, batch_size=128, epochs=5,\n",
    "          validation_split=0.1, callbacks=[EarlyStopping(monitor='val_loss', min_delta=0.001)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model and Schema\n",
    "Next step, we need to save the model and any pre-processing step you had \n",
    "on the input features (for example Categorical encoder, Tokenization, ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import shutil\n",
    "import pickle\n",
    "import yaml\n",
    "import tensorflow as tf\n",
    "\n",
    "project_id = 'tutorial'\n",
    "model_id = 'tf_ig_imdb'\n",
    "\n",
    "# create temp dir\n",
    "model_dir = pathlib.Path(model_id)\n",
    "shutil.rmtree(model_dir, ignore_errors=True)\n",
    "model_dir.mkdir()\n",
    "\n",
    "# save model\n",
    "tf.keras.experimental.export_saved_model(model, str(model_dir / 'saved_model'))\n",
    "\n",
    "# save model schema\n",
    "with open(model_dir / 'model.yaml', 'w') as yaml_file:\n",
    "    yaml.dump({'model': model_info.to_dict()}, yaml_file)\n",
    "\n",
    "# save tokenizer\n",
    "with open(model_dir / 'tokenizer.pkl', 'wb') as tok_file:\n",
    "    tok_file.write(pickle.dumps(tok))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write `package.py` and related wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import related wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to import 2 wrappers for tensorflow. Those files are stored in the utils directory.\n",
    "- The tf_saved_model_wrapper.py file contains a wrapper to load and run a TF model from a saved_model path.\n",
    "- The tf_saved_model_wrapper_ig.py file contains a wrapper to support Integrated Gradients (IG) computation for a TF model loaded from a saved_model path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['utils/tf_saved_model_wrapper.py', 'utils/tf_saved_model_wrapper_ig.py']\n",
    "for f in files:\n",
    "    shutil.copy(f, model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write `package.py` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A wrapper is needed between Fiddler and the model. This wrapper can be used to \n",
    "translate the inputs and outputs to fit what the model expects and what Fiddler \n",
    "is able to consume. This file contains functions to transform the input, generate the \n",
    "baseline and get the attributions. More information can be found [here](https://docs.fiddler.ai/api-reference/package-py/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tf_ig_imdb/package.py\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import pathlib\n",
    "import pickle\n",
    "import logging\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from .tf_saved_model_wrapper_ig import TFSavedModelWrapperIg\n",
    "\n",
    "\n",
    "PACKAGE_PATH = pathlib.Path(__file__).parent\n",
    "SAVED_MODEL_PATH = PACKAGE_PATH / 'saved_model'\n",
    "TOKENIZER_PATH = PACKAGE_PATH / 'tokenizer.pkl'\n",
    "\n",
    "LOG = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class MyModel(TFSavedModelWrapperIg):\n",
    "    def __init__(self, saved_model_path, sig_def_key, tokenizer_path,\n",
    "                 target,\n",
    "                 is_binary_classification=False,\n",
    "                 output_key=None,\n",
    "                 batch_size=8,\n",
    "                 output_columns=[],\n",
    "                 input_tensor_to_differentiable_layer_mapping={},\n",
    "                 max_allowed_error=None):\n",
    "        \"\"\"\n",
    "        Class to load and run the IMDB RNN model.\n",
    "        See: TFSavedModelWrapper\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__(saved_model_path, sig_def_key,\n",
    "                         is_binary_classification=is_binary_classification,\n",
    "                         output_key=output_key,\n",
    "                         batch_size=batch_size,\n",
    "                         output_columns=output_columns,\n",
    "                         input_tensor_to_differentiable_layer_mapping=\n",
    "                         input_tensor_to_differentiable_layer_mapping,\n",
    "                         max_allowed_error=max_allowed_error)\n",
    "        with open(tokenizer_path, 'rb') as handle:\n",
    "            self.tokenizer = pickle.load(handle)\n",
    "        self.max_seq_length = 150\n",
    "        self.target = target\n",
    "\n",
    "    def transform_input(self, input_df):\n",
    "        \"\"\"\n",
    "        Transform the provided dataframe into one that complies with the input\n",
    "        interface of the model.\n",
    "\n",
    "        Overrides the transform_input method of TFSavedModelWrapper.\n",
    "        \"\"\"\n",
    "        \n",
    "        sequences = self.tokenizer.texts_to_sequences(input_df[self.target])\n",
    "        sequences_matrix = sequence.pad_sequences(sequences,\n",
    "                                                  maxlen=self.max_seq_length,\n",
    "                                                  padding='post')\n",
    "\n",
    "        return pd.DataFrame({'inputs': sequences_matrix.tolist()})\n",
    "\n",
    "    def generate_baseline(self, input_df):\n",
    "        \n",
    "        input_tokens = input_df[self.target].apply(lambda x: '')\n",
    "        sequences = self.tokenizer.texts_to_sequences(input_tokens)\n",
    "        sequences_matrix = sequence.pad_sequences(sequences,\n",
    "                                                  maxlen=self.max_seq_length,\n",
    "                                                  padding='post')\n",
    "\n",
    "        return pd.DataFrame({'inputs': sequences_matrix.tolist()})\n",
    "\n",
    "    def project_attributions(self, input_df, transformed_input_df,\n",
    "                             attributions):\n",
    "        \"\"\"\n",
    "        Maps the transformed input to original input space so that the\n",
    "        attributions correspond to the features of the original input.\n",
    "        Overrides the project_attributions method of TFSavedModelWrapper.\n",
    "        \"\"\"\n",
    "        segments = re.split(r'([ '+self.tokenizer.filters+'])', input_df[self.target].iloc[0])\n",
    "        unpadded_input=[self.tokenizer.texts_to_sequences([x])[0] for x in input_df[self.target].values]\n",
    "        word_tokens = self.tokenizer.sequences_to_texts([[x] for x in unpadded_input[0]])\n",
    "        word_attributions = attributions['inputs'][0].astype('float').tolist()[:len(word_tokens)] \n",
    "        \n",
    "        # Let's walk segments and assign attributions to the components where\n",
    "        # they match word_tokens, the token sequence consumed by the model; otherwise assign 0.\n",
    "        i = 0\n",
    "        final_attributions = []\n",
    "        final_segments = []\n",
    "        for segment in segments:\n",
    "            if segment is not '':\n",
    "                final_segments.append(segment)\n",
    "                seg_low = segment.lower()\n",
    "                if len(word_tokens)>i and seg_low == word_tokens[i]:\n",
    "                    final_attributions.append(word_attributions[i])\n",
    "                    i+=1\n",
    "                else:\n",
    "                    final_attributions.append(0)       \n",
    "        return {\"embedding_input\":[final_segments, final_attributions]}\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    model = MyModel(\n",
    "        SAVED_MODEL_PATH,\n",
    "        tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY,\n",
    "        TOKENIZER_PATH,\n",
    "        target='sentence',\n",
    "        is_binary_classification=True,\n",
    "        batch_size=128,\n",
    "        output_columns=['inputs'],\n",
    "        input_tensor_to_differentiable_layer_mapping=\n",
    "        {'inputs': 'embedding/embedding_lookup:0'},\n",
    "        max_allowed_error=5)\n",
    "    model.load_model()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Model\n",
    "Now that we have all the parts that we need, we can go ahead and upload the model to the Fiddler platform. You can use the [upload_model_package](https://docs.fiddler.ai/api-reference/python-package/#upload-model-package) to upload this entire directory in one shot. We need the following for uploading a model:\n",
    "- The `path` to the directory\n",
    "- The `project_id` to which the model belongs\n",
    "- The `model_id`, which is the name you want to give the model. You can access it in Fiddler henceforth via this ID\n",
    "- The `dataset` which the model is linked to (optional)  \n",
    "\n",
    "In total, we will have a `model.yaml`, a `*.pkl`, and a `package.py` file within our model directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_model(project_id, model_id)\n",
    "client.upload_model_package(model_dir, project_id, model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Model\n",
    "Now, let's test out our model by interfacing with the client and \n",
    "calling [run model](https://docs.fiddler.ai/api-reference/python-package/#run-model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction_input = train_input[:10]\n",
    "result = client.run_model(project_id, model_id, prediction_input)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Explanation\n",
    "Let's get an explanation on a selected data point to better understand how our\n",
    "model came to the conclusion it did. We can do so by calling the `run_explanation`\n",
    "method. In this case, we will call for an explanation using `'ig'`.\n",
    "More information on this method can be found [here](https://docs.fiddler.ai/api-reference/python-package/#run-explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_point = df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = 'tutorial'\n",
    "model_id = 'tf_ig_imdb'\n",
    "\n",
    "ex_ig = client.run_explanation(\n",
    "    project_id=project_id,\n",
    "    model_id=model_id, \n",
    "    df=selected_point, \n",
    "    dataset_id='imdb_rnn',\n",
    "    explanations='ig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
