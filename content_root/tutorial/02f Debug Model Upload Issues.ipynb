{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Debug Model Upload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Fiddler Client\n",
    "We begin this section as usual by establishing a connection to our\n",
    "Fiddler instance. We can establish this connection either by specifying \n",
    "our credentials directly, or by utilizing our `fiddler.ini` file. More\n",
    "information can be found in the [setup](https://github.com/fiddler-labs/fiddler-samples/blob/master/content_root/tutorial/00%20Setup.ipynb) section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import fiddler as fdl\n",
    "import logging\n",
    "\n",
    "# True for logging debug message \n",
    "verbose = True\n",
    "\n",
    "if verbose:\n",
    "    logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "# client = fdl.FiddlerApi(url=url, org_id=org_id, auth_token=auth_token, verbose=verbose)\n",
    "client = fdl.FiddlerApi(verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will create a project, a convenient container for housing the models and datasets associated with a given ML use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = 'debug_model_upload'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our project using project_id\n",
    "if project_id not in client.list_projects():\n",
    "    client.create_project(project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "Here we will load in our baseline dataset from a csv called `train.csv`. We will\n",
    "also create a schema using this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/app/fiddler_samples/samples/datasets/winequality/train.csv')\n",
    "df_schema = fdl.DatasetInfo.from_dataframe(df, max_inferred_cardinality=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Dataset\n",
    "To upload a model, you first need to upload a sample of the data of the model’s \n",
    "inputs, targets, and additional metadata that might be useful for model analysis. \n",
    "This data sample helps us (among other things) to infer the model schema and the \n",
    "data types and values range of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 'wine_quality' not in client.list_datasets(project_id):\n",
    "    upload_result = client.upload_dataset(\n",
    "        project_id=project_id,\n",
    "        dataset={'train': df}, \n",
    "        dataset_id='wine_quality')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Create Model Schema\n",
    "As you must have noted, in the dataset upload step we did not ask for the model’s \n",
    "features and targets, or any model specific information. That’s because we \n",
    "allow for linking multiple models to a given dataset schema. Hence we require \n",
    "an Infer model schema step which helps us know the features relevant to the \n",
    "model and the model task. Here you can specify the input features, the target \n",
    "column, decision columns and metadata columns, and also the type of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'quality'\n",
    "train_input = df.drop(columns=['row_id', 'quality'])\n",
    "train_target = df[target]\n",
    "\n",
    "feature_columns = list(train_input.columns)\n",
    "\n",
    "model_info = fdl.ModelInfo.from_dataset_info(\n",
    "    dataset_info=client.get_dataset_info(project_id, 'wine_quality'),\n",
    "    target=target, \n",
    "    features=feature_columns,\n",
    "    display_name='debug model',\n",
    "    description='this is a sklearn model from tutorial that shows how to debug model upload'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "Build and train your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "\n",
    "\n",
    "regressor = sklearn.linear_model.LinearRegression()\n",
    "\n",
    "full_model = sklearn.pipeline.Pipeline(steps=[\n",
    "        ('standard_scaling', sklearn.preprocessing.StandardScaler()),\n",
    "        ('model_name', regressor),\n",
    "    ])\n",
    "\n",
    "full_model.fit(train_input, train_target)\n",
    "full_model.predict(train_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model and schema\n",
    "Next step, we need to save the model and any pre-processing step you had \n",
    "on the input features (for example Categorical encoder, Tokenization, ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import shutil\n",
    "import pickle\n",
    "import yaml\n",
    "\n",
    "project_id = 'tutorial'\n",
    "model_id = 'debug_model'\n",
    "\n",
    "# create temp dir\n",
    "model_dir = pathlib.Path(model_id)\n",
    "shutil.rmtree(model_dir, ignore_errors=True)\n",
    "model_dir.mkdir()\n",
    "\n",
    "# save model\n",
    "with open(model_dir / 'model.pkl', 'wb') as pkl_file:\n",
    "    pickle.dump(full_model, pkl_file)\n",
    "\n",
    "# save model schema\n",
    "with open(model_dir / 'model.yaml', 'w') as yaml_file:\n",
    "    yaml.dump({'model': model_info.to_dict()}, yaml_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write package.py wrapper\n",
    "A wrapper is needed between Fiddler and the model. This wrapper can be used to \n",
    "translate the inputs and outputs to fit what the model expects and what Fiddler \n",
    "is able to consume. More information can be found [here](https://docs.fiddler.ai/api-reference/package-py/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile debug_model/package.py\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "PACKAGE_PATH = Path(__file__).parent\n",
    "\n",
    "class SklearnModelPackage:\n",
    "    is_classifier = False\n",
    "    output_columns = ['predicted_quality']\n",
    "\n",
    "    def __init__(self):\n",
    "        with open(PACKAGE_PATH / 'model.pkl', 'rb') as infile:\n",
    "            self.model = pickle.load(infile)\n",
    "\n",
    "    def predict(self, input_df):\n",
    "        \n",
    "        # this will log the dataframe after transforming\n",
    "        logging.info(f'log dataframe {input_df}')\n",
    "        f = self.model.predict if not self.is_classifier else self.model.predict_proba\n",
    "        return pd.DataFrame(f(input_df), columns=self.output_columns)\n",
    "    \n",
    "def get_model():\n",
    "    return SklearnModelPackage()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show generated files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l debug_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test package.py locally before uploading\n",
    "You may have to restart the kernel for the class to be loaded locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from debug_model import package as model_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class.get_model().predict(train_input[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload model\n",
    "Now that we have all the parts that we need, we can go ahead and upload the model to the Fiddler platform. You can use the [upload_model_package](https://docs.fiddler.ai/api-reference/python-package/#upload-model-package) to upload this entire directory in one shot. We need the following for uploading a model:\n",
    "- The `path` to the directory\n",
    "- The `project_id` to which the model belongs\n",
    "- The `model_id`, which is the name you want to give the model. You can access it in Fiddler henceforth via this ID\n",
    "- The `dataset` which the model is linked to (optional)  \n",
    "\n",
    "In total, we will have a `model.yaml`, a `*.pkl`, and a `package.py` file within our model directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_model(project_id, model_id)\n",
    "client.upload_model_package(model_dir, project_id, model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run model\n",
    "Now, let's test out our model by interfacing with the client and \n",
    "calling [run model](https://docs.fiddler.ai/api-reference/python-package/#run-model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_input = train_input[0: 10]\n",
    "result = client.run_model(project_id, model_id, prediction_input, log_events=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
