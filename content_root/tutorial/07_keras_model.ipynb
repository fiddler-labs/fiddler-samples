{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup: make sure you have the right versions of packages\n",
    "You may need to restart the kernel after running this cell for the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already up-to-date: tensorflow==1.14.* in /home/jupyterdocker/.local/lib/python3.7/site-packages (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /home/jupyterdocker/.local/lib/python3.7/site-packages (from tensorflow==1.14.*) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /home/jupyterdocker/.local/lib/python3.7/site-packages (from tensorflow==1.14.*) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /home/jupyterdocker/.local/lib/python3.7/site-packages (from tensorflow==1.14.*) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard<1.15.0,>=1.14.0 in /home/jupyterdocker/.local/lib/python3.7/site-packages (from tensorflow==1.14.*) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.*) (0.34.2)\n",
      "Requirement already satisfied, skipping upgrade: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /home/jupyterdocker/.local/lib/python3.7/site-packages (from tensorflow==1.14.*) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.14.5 in /home/jupyterdocker/.local/lib/python3.7/site-packages (from tensorflow==1.14.*) (1.18.1)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /home/jupyterdocker/.local/lib/python3.7/site-packages (from tensorflow==1.14.*) (3.11.3)\n",
      "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /home/jupyterdocker/.local/lib/python3.7/site-packages (from tensorflow==1.14.*) (0.9.0)\n",
      "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /home/jupyterdocker/.local/lib/python3.7/site-packages (from tensorflow==1.14.*) (1.27.2)\n",
      "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /home/jupyterdocker/.local/lib/python3.7/site-packages (from tensorflow==1.14.*) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /home/jupyterdocker/.local/lib/python3.7/site-packages (from tensorflow==1.14.*) (0.1.8)\n",
      "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /home/jupyterdocker/.local/lib/python3.7/site-packages (from tensorflow==1.14.*) (0.8.1)\n",
      "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /home/jupyterdocker/.local/lib/python3.7/site-packages (from tensorflow==1.14.*) (1.0.8)\n",
      "Requirement already satisfied, skipping upgrade: gast>=0.2.0 in /home/jupyterdocker/.local/lib/python3.7/site-packages (from tensorflow==1.14.*) (0.2.2)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.*) (45.1.0)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /home/jupyterdocker/.local/lib/python3.7/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.*) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /home/jupyterdocker/.local/lib/python3.7/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.*) (3.2.1)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /home/jupyterdocker/.local/lib/python3.7/site-packages (from keras-applications>=1.0.6->tensorflow==1.14.*) (2.10.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyterdocker/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jupyterdocker/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jupyterdocker/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jupyterdocker/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jupyterdocker/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jupyterdocker/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/jupyterdocker/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jupyterdocker/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jupyterdocker/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jupyterdocker/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jupyterdocker/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jupyterdocker/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 1.14.0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already up-to-date: scikit-learn==0.21.* in /home/jupyterdocker/.local/lib/python3.7/site-packages (0.21.3)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /home/jupyterdocker/.local/lib/python3.7/site-packages (from scikit-learn==0.21.*) (0.14.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.0 in /home/jupyterdocker/.local/lib/python3.7/site-packages (from scikit-learn==0.21.*) (1.18.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.17.0 in /home/jupyterdocker/.local/lib/python3.7/site-packages (from scikit-learn==0.21.*) (1.4.1)\n"
     ]
    }
   ],
   "source": [
    "# ensure TF 1 not TF 2 (use version corresponding to what's in your Fiddler configuration, 1.14 as of 2020-02-26)\n",
    "!pip install --upgrade tensorflow==1.14.*\n",
    "\n",
    "import tensorflow as tf\n",
    "# NOTE: we can only run tf.saved_model.save() on a tf.keras model if we use eager execution\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "# triple-check version is 1.x\n",
    "print('TF version:', tf.__version__)\n",
    "assert tf.__version__[0] == '1', 'Stop! This tutorial is meant to use TF 1.x!'\n",
    "\n",
    "# likewise, make sure the scikit-learn version matches what's in Fidler (0.21 as of 2020-02-26)\n",
    "!pip install --upgrade scikit-learn==0.21.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import pathlib\n",
    "import pickle\n",
    "import shutil\n",
    "import time\n",
    "import zipfile\n",
    "\n",
    "import category_encoders\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sklearn.metrics\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "\n",
    "import fiddler as fdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "In tutorial_03, we covered advanced model upload in Tensorflow. This tutorial is a small extension demonstrating the Keras equivalent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 0: Configure connection to Fiddler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: typically the API url for your running instance of Fiddler will beÂ \"https://api.fiddler.ai\" (or \"http://localhost:4100\" for onebox)\n",
    "# however, use \"http://host.docker.internal:4100\" as our URL if Jupyter is running in a docker VM on the same macOS machine as onebox\n",
    "url = 'http://localhost:4100'\n",
    "\n",
    "# see <Fiddler URL>/settings/credentials to find, create, or change this token\n",
    "token = os.getenv('FIDDLER_API_TOKEN')\n",
    "\n",
    "# see <Fiddler URL>/settings/general to find this id (listed as \"Organization Name\")\n",
    "org_id = 'onebox'\n",
    "\n",
    "fiddler_api = fdl.FiddlerApi(url=url, org_id=org_id, auth_token=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Preparing data and model.\n",
    "For a more step-by-step version of this section, see tutorial_03."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Heads up! We are inferring the details of your dataset from the dataframe(s) provided. Please take a second to check our work.\n",
      "\n",
      "If the following DatasetInfo is an incorrect representation of your data, you can construct a DatasetInfo with the DatasetInfo.from_dataframe() method and modify that object to reflect the correct details of your dataset.\n",
      "\n",
      "After constructing a corrected DatasetInfo, please re-upload your dataset with that DatasetInfo object explicitly passed via the `info` parameter of FiddlerApi.upload_dataset().\n",
      "\n",
      "You may need to delete the initially uploaded versionvia FiddlerApi.delete_dataset('bikeshare').\n",
      "\n",
      "Inferred DatasetInfo to check:\n",
      "  DatasetInfo:\n",
      "    display_name: bikeshare\n",
      "    files: []\n",
      "    columns:\n",
      "              column     dtype count(possible_values)\n",
      "      0       dteday    STRING                      -\n",
      "      1       season  CATEGORY                      4\n",
      "      2           yr   INTEGER                      -\n",
      "      3         mnth   INTEGER                      -\n",
      "      4           hr   INTEGER                      -\n",
      "      5      holiday   BOOLEAN                      -\n",
      "      6      weekday   INTEGER                      -\n",
      "      7   workingday   BOOLEAN                      -\n",
      "      8   weathersit  CATEGORY                      4\n",
      "      9         temp     FLOAT                      -\n",
      "      10       atemp     FLOAT                      -\n",
      "      11         hum     FLOAT                      -\n",
      "      12   windspeed     FLOAT                      -\n",
      "      13      casual   INTEGER                      -\n",
      "      14  registered   INTEGER                      -\n",
      "      15         cnt   INTEGER                      -\n",
      "Heads up! We are inferring the details of your dataset from the dataframe(s) provided. Please take a second to check our work.\n",
      "\n",
      "If the following DatasetInfo is an incorrect representation of your data, you can construct a DatasetInfo with the DatasetInfo.from_dataframe() method and modify that object to reflect the correct details of your dataset.\n",
      "\n",
      "After constructing a corrected DatasetInfo, please re-upload your dataset with that DatasetInfo object explicitly passed via the `info` parameter of FiddlerApi.upload_dataset().\n",
      "\n",
      "You may need to delete the initially uploaded versionvia FiddlerApi.delete_dataset('bikeshare_processed').\n",
      "\n",
      "Inferred DatasetInfo to check:\n",
      "  DatasetInfo:\n",
      "    display_name: bikeshare_processed\n",
      "    files: []\n",
      "    columns:\n",
      "                column    dtype count(possible_values)\n",
      "      0       season_1    FLOAT                      -\n",
      "      1       season_2    FLOAT                      -\n",
      "      2       season_3    FLOAT                      -\n",
      "      3       season_4    FLOAT                      -\n",
      "      4           mnth    FLOAT                      -\n",
      "      5             hr    FLOAT                      -\n",
      "      6        holiday    FLOAT                      -\n",
      "      7        weekday    FLOAT                      -\n",
      "      8     workingday    FLOAT                      -\n",
      "      9   weathersit_1    FLOAT                      -\n",
      "      10  weathersit_2    FLOAT                      -\n",
      "      11  weathersit_3    FLOAT                      -\n",
      "      12  weathersit_4    FLOAT                      -\n",
      "      13          temp    FLOAT                      -\n",
      "      14         atemp    FLOAT                      -\n",
      "      15           hum    FLOAT                      -\n",
      "      16     windspeed    FLOAT                      -\n",
      "      17           cnt  INTEGER                      -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing all datasets in Fiddler, you should see \"bikeshare\" and \"bikeshare_processed\" in this list.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['time_split_dataset',\n",
       " '20news',\n",
       " 'titanic',\n",
       " 'iris',\n",
       " 'bank_churn',\n",
       " 'bikeshare_processed',\n",
       " 'bikeshare',\n",
       " 'imdb_rnn',\n",
       " 'winequality',\n",
       " 'p2p_loans']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip'\n",
    "z = zipfile.ZipFile(io.BytesIO(requests.get(zip_url).content))\n",
    "\n",
    "# here we pre-configure the datatypes for our dataframe\n",
    "# so it doesn't require any datatype modification after import\n",
    "bikeshare_dtypes = dict(season='category', holiday='bool',\n",
    "                        workingday='bool', weathersit='category')\n",
    "bikeshare_datetime_columns = ['dteday']\n",
    "bikeshare_index_column = 'instant'\n",
    "with z.open('hour.csv') as csv:\n",
    "    df = pd.read_csv(csv, \n",
    "                     dtype=bikeshare_dtypes, \n",
    "                     parse_dates=bikeshare_datetime_columns,\n",
    "                     index_col=bikeshare_index_column)\n",
    "\n",
    "# split train/test by year\n",
    "is_2011 = df['yr'] == 0\n",
    "df_2011 = df[is_2011].reset_index(drop=True)\n",
    "df_2012 = df[~is_2011].reset_index(drop=True)\n",
    "\n",
    "# specify which columns are features and which are not\n",
    "target = 'cnt'\n",
    "not_used_as_features = ['dteday', 'yr', 'casual', 'registered']\n",
    "non_feature_columns = [target] + not_used_as_features\n",
    "feature_columns = list(set(df_2011.columns) - set(non_feature_columns))\n",
    "\n",
    "# split our data into features and targets\n",
    "x_train = df_2011.drop(columns=non_feature_columns)\n",
    "x_test = df_2012.drop(columns=non_feature_columns)\n",
    "y_train = df_2011[target]\n",
    "y_test = df_2012[target]\n",
    "\n",
    "onehot = category_encoders.OneHotEncoder(cols=df.select_dtypes('category').columns.tolist())\n",
    "standard_scaler = sklearn.preprocessing.StandardScaler()\n",
    "preprocessor = sklearn.pipeline.make_pipeline(onehot, standard_scaler)\n",
    "preprocessor.fit(x_train)\n",
    "x_train_processed = preprocessor.transform(x_train)\n",
    "x_test_processed = preprocessor.transform(x_test)\n",
    "\n",
    "# delete the datasets if we've uploaded them previously\n",
    "try:\n",
    "    fiddler_api.delete_dataset('bikeshare')\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    fiddler_api.delete_dataset('bikeshare_processed')\n",
    "except:\n",
    "    pass\n",
    "time.sleep(2)\n",
    "\n",
    "# let's upload the original dataset\n",
    "fiddler_api.upload_dataset(\n",
    "    dataset={'train': df_2011, 'test': df_2012}, \n",
    "    dataset_id='bikeshare')\n",
    "\n",
    "# let's also upload the preprocessed version of the dataset\n",
    "df_2011_processed = pd.concat([pd.DataFrame(x_train_processed, columns=onehot.feature_names), y_train], axis=1)\n",
    "df_2012_processed = pd.concat([pd.DataFrame(x_test_processed, columns=onehot.feature_names), y_test], axis=1)\n",
    "fiddler_api.upload_dataset(\n",
    "    dataset={'train': df_2011_processed, 'test': df_2012_processed}, \n",
    "    dataset_id='bikeshare_processed')\n",
    "\n",
    "print('Listing all datasets in Fiddler, you should see \"bikeshare\" and \"bikeshare_processed\" in this list.')\n",
    "fiddler_api.list_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Building a Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "8645/8645 [==============================] - 1s 94us/sample - loss: 12530.6890\n",
      "Epoch 2/2\n",
      "8645/8645 [==============================] - 1s 110us/sample - loss: 10400.1665\n",
      "Epoch 1/8\n",
      "8645/8645 [==============================] - 0s 55us/sample - loss: 9662.6996\n",
      "Epoch 2/8\n",
      "8645/8645 [==============================] - 0s 45us/sample - loss: 7945.1241\n",
      "Epoch 3/8\n",
      "8645/8645 [==============================] - 0s 47us/sample - loss: 6275.6257\n",
      "Epoch 4/8\n",
      "8645/8645 [==============================] - 0s 44us/sample - loss: 4784.9374\n",
      "Epoch 5/8\n",
      "8645/8645 [==============================] - 0s 48us/sample - loss: 3639.8403\n",
      "Epoch 6/8\n",
      "8645/8645 [==============================] - 0s 42us/sample - loss: 2974.1145\n",
      "Epoch 7/8\n",
      "8645/8645 [==============================] - 0s 44us/sample - loss: 2671.0021\n",
      "Epoch 8/8\n",
      "8645/8645 [==============================] - 0s 47us/sample - loss: 2327.8333\n",
      "The model achieves a test-set r2 score of 0.58\n"
     ]
    }
   ],
   "source": [
    "# Train a 2-layer MLP model\n",
    "inputs = tf.keras.Input(shape=(x_train_processed.shape[1],))\n",
    "activations = tf.keras.layers.Dense(128, activation=tf.nn.relu)(inputs)\n",
    "activations = tf.keras.layers.Dense(128, activation=tf.nn.relu)(activations)\n",
    "activations = tf.keras.layers.Dense(1)(activations)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=activations, name='keras_bikeshare_mlp_model')\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.003),\n",
    "    loss='mse')\n",
    "model.fit(x_train_processed, y_train.values, batch_size=16, epochs=2)\n",
    "model.optimizer.learning_rate = 0.01\n",
    "model.fit(x_train_processed, y_train.values, batch_size=32, epochs=8)\n",
    "\n",
    "y_hat = model.predict(x_test_processed)\n",
    "r2 = sklearn.metrics.r2_score(y_test, y_hat)\n",
    "print(f'The model achieves a test-set r2 score of {r2:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Uploading Keras model to Fiddler\n",
    "Here we adapt the previous tutorial to use a Keras .h5 saved model format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project already exists, no change.\n",
      "Running on Fiddler, the model predicts 39.31 for the first example row, locally running the model before gave 39.31!\n"
     ]
    }
   ],
   "source": [
    "from fiddler.model_loaders import KerasModel\n",
    "\n",
    "# preliminary info\n",
    "project_id = 'bikeshare_forecasting'\n",
    "model_id = 'raw_features_mlp'\n",
    "tf_model = model\n",
    "example_data = pd.concat([x_test, y_test], axis=1)\n",
    "target_column_name = 'cnt'\n",
    "model_dir = pathlib.Path('tf_model')\n",
    "\n",
    "# create a ModelInfo\n",
    "model_info = fdl.ModelInfo.from_dataset_info(\n",
    "    dataset_info=fdl.DatasetInfo.from_dataframe(example_data),\n",
    "    target=target_column_name,\n",
    "    features=example_data.columns.difference([target_column_name]).tolist(),\n",
    "    input_type=fdl.ModelInputType.TABULAR,\n",
    ")\n",
    "\n",
    "# (re-)create directory for the model\n",
    "shutil.rmtree(model_dir, ignore_errors=True)\n",
    "model_dir.mkdir()\n",
    "\n",
    "# save the preprocessor\n",
    "with (model_dir / 'preprocessor.pkl').open('wb') as pkl_file:\n",
    "    pickle.dump(preprocessor, pkl_file)\n",
    "\n",
    "# save the tensorflow model\n",
    "model.save(str(model_dir / 'model.h5'), include_optimizer=False)\n",
    "\n",
    "# create package.py\n",
    "package_py_contents = '''\n",
    "import pathlib\n",
    "import pickle\n",
    "\n",
    "import yaml\n",
    "\n",
    "import fiddler as fdl\n",
    "from fiddler.model_loaders import KerasModel\n",
    "\n",
    "MODEL_DIR = pathlib.Path(__file__).parent\n",
    "\n",
    "def load_model_info(model_dir):\n",
    "    \"\"\"Load ModelInfo from a model.yaml file\"\"\"\n",
    "    with (pathlib.Path(model_dir) / 'model.yaml').open('r') as yaml_file:\n",
    "        return fdl.ModelInfo.from_dict(yaml.load(yaml_file, Loader=yaml.SafeLoader))\n",
    "\n",
    "def get_model(model_dir=MODEL_DIR, tf_saved_model_dir=MODEL_DIR / 'model.h5'):\n",
    "    model_info = load_model_info(model_dir)\n",
    "    output_names = model_info.get_output_names()\n",
    "    is_binary_classification = (\n",
    "        model_info.model_task.name \n",
    "            == fdl.ModelTask.BINARY_CLASSIFICATION.name\n",
    "    )\n",
    "    # load preprocessor before we define custom_input_transformation()\n",
    "    with (MODEL_DIR / 'preprocessor.pkl').open('rb') as pkl_file:\n",
    "        preproc = pickle.load(pkl_file)\n",
    "\n",
    "    def custom_input_transformation(input_df):\n",
    "        return [preproc.transform(input_df)]\n",
    "    \n",
    "    return KerasModel(\n",
    "        tf_saved_model_dir, \n",
    "        output_column_names=output_names,\n",
    "        is_binary_classification=is_binary_classification,\n",
    "        input_transformation=custom_input_transformation,\n",
    "    )\n",
    "'''\n",
    "with (model_dir / 'package.py').open('w') as f:\n",
    "    f.write(package_py_contents)\n",
    "\n",
    "# (re-)upload our model\n",
    "fiddler_api.create_project(project_id)\n",
    "if model_id in fiddler_api.list_models(project_id):\n",
    "    fiddler_api.delete_model(project_id, model_id)\n",
    "time.sleep(2)\n",
    "fiddler_api.upload_model_custom(\n",
    "    artifact_path=model_dir, \n",
    "    info=model_info, \n",
    "    project_id=project_id, \n",
    "    model_id=model_id,\n",
    "    associated_dataset_ids=['bikeshare']\n",
    ")\n",
    "\n",
    "# clean up local directory\n",
    "shutil.rmtree(model_dir, ignore_errors=True)\n",
    "\n",
    "# verify the uploaded model runs\n",
    "time.sleep(2)\n",
    "pred = fiddler_api.run_model(project_id, model_id, example_data.head(1))\n",
    "print(f'Running on Fiddler, the model predicts {pred.iat[0,0]:.2f} for the first example row, locally running the model before gave {float(y_hat[0]):.2f}!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
