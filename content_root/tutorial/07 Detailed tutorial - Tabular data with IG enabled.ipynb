{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Fiddler Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This python client is a powerful way to:\n",
    "- Upload the dataset and model to Fiddler\n",
    "- Ingest production events to Fiddler\n",
    "\n",
    "This can be done from a Jupyter Notebook or any python editor that you use to load data and build models.\n",
    "\n",
    "<img src=\"images/fiddler_client.png\" width=600 height=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to initialize the client object by specifying:\n",
    "- The url: url is the fiddler URL that you have been provided to access. Usually of the form ‘XXXXX.fiddler.ai’. Contact Fiddler if you don’t have it\n",
    "- The org_id: organization id is an identifier for the account. See Fiddler_URL/settings/general to find this id (listed as \"Organization ID\")\n",
    "<img src=\"images/org_id.png\" width=800 height=800 />\n",
    "- The auth_token: this token is used to authenticate access. See Fiddler_URL/settings/credentials to find, create, or change this token\n",
    "<img src=\"images/auth_token.png\" width=800 height=800 />\n",
    "\n",
    "You can also save this config as a file called fiddler.ini in the same folder as the notebook/script. That saves you from specifying the parameters in every notebook and script.\n",
    "<img src=\"images/fiddler_ini.png\" width=800 height=800 />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiddler as fdl\n",
    "\n",
    "url = 'http://xxx.fiddler.ai'\n",
    "token = 'my_token'\n",
    "org_id = 'my_org_id'\n",
    "\n",
    "client = fdl.FiddlerApi(url=url, org_id=org_id, auth_token=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fiddler has three primary constructs, namely projects, datasets and models. This diagram illustrates the relationship between the three.\n",
    "<img src=\"images/projects_data_models.png\" width=600 height=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Fiddler client provides a number of methods.\n",
    "- List datasets: ```client.list_datasets()``` List the ids of all datasets in the org.\n",
    "- List projects: ```client.list_projects()``` List the ids of all projects in the org.\n",
    "- List models: ```client.list_models()``` List the names of all models in a project.\n",
    "- Create project: ```client.create_project()``` Create a new project.\n",
    "- Create model: ```client.create_model()``` Trigger auto-modeling on a dataset already uploaded to Fiddler.\n",
    "- Get dataset info: ```client.get_dataset_info()``` Get DatasetInfo for a dataset.\n",
    "- Get model info: ```client.get_model_info()``` Get ModelInfo for a model in a certain project.\n",
    "- Get dataset: ```client.get_dataset()``` Fetches data from a dataset on Fiddler.\n",
    "- Get slice: ```client.get_slice()``` Fetches data from Fiddler via a slice query (SQL query).\n",
    "- Delete dataset: ```client.delete_dataset()``` Permanently delete a dataset.\n",
    "- Delete model: ```client.delete_model()``` Permanently delete a model.\n",
    "- Delete model artifacts: ```client.delete_model_artifacts()``` Permanently delete a model artifacts.\n",
    "- Delete project: ```client.delete_project()``` Permanently delete a project.\n",
    "- Upload dataset: ```client.upload_dataset()``` Uploads a dataset to the Fiddler engine.\n",
    "- Upload dataset from a directory: ```client.upload_dataset_from_dir()``` Uploads a dataset from a directory to the Fiddler engine.\n",
    "- Run model: ```client.run_model()``` Executes a model in the Fiddler engine on a DataFrame.\n",
    "- Run explanation: ```client.run_explanation()``` Explains a model's prediction on a single instance.\n",
    "- Run feature importance: ```client.run_feature_importance()``` Get global feature importance for a model over a dataset.\n",
    "- Upload model sklearn: ```client.upload_model_sklearn()``` Uploads a subclass of sklearn.base.BaseEstimator to the Fiddler engine.\n",
    "- Upload model package: ```client.upload_model_package()``` Uploads a custom model object to the Fiddler engine along with custom glue-code for running the model.\n",
    "- Publish event: ```client.publish_event()``` Publishes an event to Fiddler Service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data you are going to use for training your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/app/fiddler_samples/samples/datasets/heart_disease/data.csv')\n",
    "df_schema = fdl.DatasetInfo.from_dataframe(df, max_inferred_cardinality=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To upload a model, you first need to upload a sample of the data of the model’s inputs, targets, and additional metadata that might be useful for model analysis. This data sample helps us (among other things) to infer the model schema and the data types and values range of each feature.\n",
    "- This sample has to be a flat table that can be loaded as a pandas DF (```upload_dataset()```) or saved as a csv (```upload_dataset_from_dir()```).\n",
    "- In this example age, sex, trestbps, chol, fbs, thalach, exang, oldpeak, slope are input features, and target is the target column for the model.\n",
    "- This input data sample is used for many downstream functions in Fiddler\n",
    "    - Shapley value methods - background data to simulate the missing of features\n",
    "    - What-if (ICE) plots - background data\n",
    "    - PDP plots - background data\n",
    "    - Drift - to serve as a baseline\n",
    "    - Outliers - to serve as a baseline\n",
    "    - Data integrity - to serve as a baseline\n",
    "- We suggest uploading a sample of the model’s training data as it’s the most meaningful for the tasks listed above. For example, model outliers should be ideally based on the training data as that’s the data the model has seen. \n",
    "- You can upload multiple datasets with string identifiers, but we currently do not ascribe any meaning to those. For example: ```dataset={'data': df}``` or ```dataset={'train': train_df, 'test': test_df}```.\n",
    "- Currently we support two input types:\n",
    "    - Tabular\n",
    "    - Single string text, meaning text data in a single column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'heart_disease' not in client.list_datasets():\n",
    "    upload_result = client.upload_dataset(\n",
    "        dataset={'data': df}, \n",
    "        dataset_id='heart_disease')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To organize our models, let's first create a project on Fiddler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = 'tutorial'\n",
    "\n",
    "if project_id not in client.list_projects():\n",
    "    client.create_project(project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you must have noted, in the dataset upload step we did not ask for the model’s features and targets, or any model specific information. That’s because we allow for linking multiple models to a given dataset schema. Hence we require an Infer model schema step which helps us know the features relevant to the model and the model task. Here you can specify the input features, the target column, decision columns and metadata columns, and also the type of model.\n",
    "- Currently we support only one target column. This is not to be confused with output columns, which can be more than one. \n",
    "- Decision columns specify the decisions made on the basis of the model’s predictions. For example, in a credit lending scenario, the business decision to give or not to give a loan based on the model’s output. This is helpful while monitoring models after deployment, to keep track of the business impact of the model.\n",
    "- Metadata is data that is not used by the model, but can be relevant for understanding the model’s behavior on different segments of the data. For example, gender, race, age and other such sensitive features may not be used in the model, but we can analyze along these dimensions post facto to understand if the model is biased.\n",
    "- We can infer the model task from the target column, or it can explicitly set. Currently we support three model types:\n",
    "    - Regression\n",
    "    - Binary Classification\n",
    "    - Multi-class Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'target'\n",
    "train_input = df.drop(columns=['target'])\n",
    "train_target = df[target]\n",
    "\n",
    "feature_columns = list(train_input.columns)\n",
    "\n",
    "model_info = fdl.ModelInfo.from_dataset_info(\n",
    "    dataset_info=client.get_dataset_info('heart_disease'),\n",
    "    target=target, \n",
    "    features=feature_columns,\n",
    "    display_name='Keras Tabular IG',\n",
    "    description='this is a keras model using tabular data and IG enabled from tutorial',\n",
    "    model_task=fdl.ModelTask.BINARY_CLASSIFICATION\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install TensorFlow if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, we support Sklearn version 0.21.2 and TF version 1.14  \n",
    "If you have another version, please contact Fiddler for assistance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow==1.14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and train your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "inputs = tf.keras.Input(shape=(train_input.shape[1], ))\n",
    "activations = tf.keras.layers.Dense(32, activation='linear', use_bias=True)(inputs)\n",
    "activations = tf.keras.layers.Dense(128, activation=tf.nn.relu, use_bias=True)(activations)\n",
    "activations = tf.keras.layers.Dense(128, activation=tf.nn.relu, use_bias=True)(activations)\n",
    "activations = tf.keras.layers.Dense(1, activation='sigmoid', use_bias=True)(activations)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=activations, name='keras_model')\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(train_input, train_target.values, batch_size=32, epochs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(train_input, train_target) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model and schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step, we need to save the model and any pre-processing step you had on the input features (for example Categorical encoder, Tokenization, ...).  \n",
    "We currently support the following stored model formats:\n",
    "- For sklearn API based models, pickled models, or any storage format that you can load in the package.py (details below).\n",
    "- For TF, we support TF Saved Model and Keras .h5   \n",
    "\n",
    "Note:\n",
    "- Keras models have to have their input tensor differentiable if Integrated Gradients support is desired\n",
    "- We also need to save the data preprocessing pipeline code, if any. This will be accessed in the package.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import shutil\n",
    "import yaml\n",
    "\n",
    "# Let's save the model in the TF Saved Model format\n",
    "model_id_tf = 'heart_disease_tf'\n",
    "\n",
    "# create temp dir\n",
    "model_dir_tf = pathlib.Path(model_id_tf)\n",
    "shutil.rmtree(model_dir_tf, ignore_errors=True)\n",
    "model_dir_tf.mkdir()\n",
    "\n",
    "# save model\n",
    "tf.saved_model.save(model, str(model_dir_tf / 'saved_model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demo purpose, let's save this model in Keras .h5 demo.\n",
    "model_id_keras = 'heart_disease_keras'\n",
    "\n",
    "# create temp dir\n",
    "model_dir_keras = pathlib.Path(model_id_keras)\n",
    "shutil.rmtree(model_dir_keras, ignore_errors=True)\n",
    "model_dir_keras.mkdir()\n",
    "\n",
    "# save model\n",
    "model.save(str(model_dir_keras / 'model.h5'), include_optimizer=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following section, we are providing the code to upload your model in the two saved format supported. Please refer to the appropriate section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write package.py and related wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. For TF Saved models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import related wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to import 2 wrappers for tensorflow. Those files are stored in the utils directory.\n",
    "- The tf_saved_model_wrapper.py file contains a wrapper to load and run a TF model from a saved_model path.\n",
    "- The tf_saved_model_wrapper_ig.py file contains a wrapper to support Integrated Gradients (IG) computation for a TF model loaded from a saved_model path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['utils/tf_saved_model_wrapper.py', 'utils/tf_saved_model_wrapper_ig.py']\n",
    "for f in files:\n",
    "    shutil.copy(f, model_dir_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Package.py is the interface between Fiddler’s backend and your model. This code helps Fiddler to understand the model, its inputs and outputs.\n",
    "- Load the model, and any associated files such as feature transformers or tokenizers.\n",
    "- Transform the data into a format that the model recognizes.\n",
    "- Make batch predictions using the model.\n",
    "- Understand the differentiable tensors of the model, in case we want to enable Integrated Gradients.\n",
    "\n",
    "For certain common highly standardized frameworks, the Fiddler client provides helper upload methods to auto-generate this module (e.g. for scikit-learn models use ```upload_model_sklearn()```).\n",
    "\n",
    "\n",
    "Writting the package.py file:\n",
    "- package.py will be invoked within the model’s specific assets directory and must implement a get_model() function which takes no arguments and returns an instance of a model class implementing the following methods:\n",
    "    - The initialization parameters For TF models:\n",
    "        - ```self.max_allowed_error```: Float specifying a percentage value for the maximum allowed integral approximation error for IG computation. If None then IG will be  calculated for a pre-determined number of steps. Otherwise, the number of steps will be increased till the error is within the specified limit.\n",
    "        - ```self.model```: the code to load the model in the given session, you need to specify the file name essentially.\n",
    "        - ```self.output_columns```: a list of names of the output columns for the model.\n",
    "        - ```self.batch_size```: set a batch size for the model which will not cause OOM errors on the machines(s) the Fiddler cluster is hosted on. For the machine’s configuration, please check with Fiddler.\n",
    "        - ```self.ig_enabled```: if you want the Integrated gradients explanation method for your model. If False, then you can skip all the below parameters\n",
    "    - ```transform_input(input_df)```: Accepts a pandas DataFrame object containing rows of raw feature vectors. The output of this method can be any Python object. This function can also be used to deserialize complex data types stored in dataset columns (e.g. images stored in a field in UTF-8 format). This function is typically called by predict, but the platform may also need to invoke it directly for certain operations (e.g. computing path integral steps in the Integrated Gradients explanation method).\n",
    "    - ```generate_baseline(input_df)```: Generates a DataFrame specifying a baseline that is required for\n",
    "    calculating Integrated Gradients. The Baseline is a certain 'informationless' input relative to which\n",
    "    attributions must be computed. For instance, in a text classification model, the baseline could be the empty         text.mThe baseline could be the same for all inputs or could be specific to the input at hand. \n",
    "    The choice of baseline is important as explanations are contextual to a baseline. For more information please         refer to the following document:\n",
    "    https://github.com/ankurtaly/Integrated-Gradients/blob/master/howto.md\n",
    "    - ```predict(input_df)```: Accepts a pandas DataFrame object containing rows of raw feature vectors. Outputs a pandas DataFrame object containing the model predictions whose column labels must match the output column names in model.yaml. Typically this function invokes transform_input explicitly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile heart_disease_tf/package.py\n",
    "\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from .tf_saved_model_wrapper_ig import TFSavedModelWrapperIg\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "PACKAGE_PATH = pathlib.Path(__file__).parent\n",
    "SAVED_MODEL_PATH = PACKAGE_PATH / 'saved_model'\n",
    "\n",
    "class MyModel(TFSavedModelWrapperIg):\n",
    "    \"\"\"\n",
    "    :param saved_model_path: Path to the directory containing the TF\n",
    "       model in SavedModel format.\n",
    "       See: https://www.tensorflow.org/guide/saved_model#build_and_load_a_savedmodel\n",
    "\n",
    "    :param sig_def_key: Key for the specific SignatureDef to be used for\n",
    "       executing the model.\n",
    "       See: https://www.tensorflow.org/tfx/serving/signature_defs#signaturedef_structure\n",
    "\n",
    "    :param output_columns: List containing the names of the output\n",
    "       column(s) that corresponds to the output of the model. If the\n",
    "       model is a binary classification model then the number of output\n",
    "       columns is one, otherwise, the number of columns must match the\n",
    "       shape of the output tensor corresponding to the output key\n",
    "       specified.\n",
    "\n",
    "    :param is_binary_classification [optional]: Boolean specifying if the\n",
    "       model is a binary classification model. If True, the number of\n",
    "       output columns is one. The default is False.\n",
    "\n",
    "    :param output_key [optional]: Key for the specific output tensor (\n",
    "       specified in the SignatureDef) whose predictions must be explained.\n",
    "       The output tensor must specify a differentiable output of the\n",
    "       model. Thus, output tensors that are generated as a result of\n",
    "       discrete operations (e.g., argmax) are disallowed. The default is\n",
    "       None, in which case the first output listed in the SignatureDef is\n",
    "       used. The 'saved_model_cli' can be used to view the output tensor\n",
    "       keys available in the signature_def.\n",
    "       See: https://www.tensorflow.org/guide/saved_model#cli_to_inspect_and_execute_savedmodel\n",
    "\n",
    "    :param batch_size [optional]: the batch size for input into the model.\n",
    "       Depends on model and instance config.\n",
    "\n",
    "    :param input_tensor_to_differentiable_layer_mapping [optional]:\n",
    "       Dictionary that maps input tensors to the first differentiable\n",
    "       layer/tensor in the graph they are attached to. For instance,\n",
    "       in a text model, an input tensor containing token ids\n",
    "       may not be differentiable but may feed into an embedding tensor.\n",
    "       Such an input tensor must be mapped to the corresponding the\n",
    "       embedding tensor in this dictionary.\n",
    "\n",
    "       All input tensors must be mentioned in the dictionary. An input\n",
    "       tensor that is directly differentiable may be mapped to itself.\n",
    "\n",
    "       For each differentiable tensor, the first dimension must be the\n",
    "       batch dimension. If <k1, …, kn> is the shape of the input then the\n",
    "       differentiable tensor must either have the same shape or the shape\n",
    "       <k1, …, kn, d>.\n",
    "\n",
    "       The default is None, in which case all input tensors are assumed\n",
    "       to be differentiable.\n",
    "\n",
    "    :param max_allowed_error: Float specifying a percentage value\n",
    "       for the maximum allowed integral approximation error for IG\n",
    "       computation. If None then IG will be  calculated for a\n",
    "       pre-determined number of steps. Otherwise, the number of steps\n",
    "       will be increased till the error is within the specified limit.\n",
    "    \"\"\"\n",
    "    def __init__(self, saved_model_path, sig_def_key,\n",
    "                 is_binary_classification=False,\n",
    "                 output_key=None,\n",
    "                 batch_size=8,\n",
    "                 output_columns=[],\n",
    "                 input_tensor_to_differentiable_layer_mapping={},\n",
    "                 max_allowed_error=None):\n",
    "        \n",
    "        super().__init__(saved_model_path, sig_def_key,\n",
    "                         is_binary_classification=is_binary_classification,\n",
    "                         output_key=output_key,\n",
    "                         batch_size=batch_size,\n",
    "                         output_columns=output_columns,\n",
    "                         input_tensor_to_differentiable_layer_mapping=\n",
    "                         input_tensor_to_differentiable_layer_mapping,\n",
    "                         max_allowed_error=max_allowed_error)\n",
    "\n",
    "\n",
    "    def transform_input(self, input_df):\n",
    "        \"\"\"\n",
    "        Transform the provided pandas DataFrame into one that complies with\n",
    "        the input interface of the model. This method returns a pandas\n",
    "        DataFrame with columns corresponding to the input tensor keys in the\n",
    "        SavedModel SignatureDef. The contents of each column match the input\n",
    "        tensor shape described in the SignatureDef.\n",
    "\n",
    "        Args:\n",
    "        :param input_df: DataFrame corresponding to the dataset yaml\n",
    "            associated with the project. Specifically, the columns in the\n",
    "            DataFrame must correspond to the feature names mentioned in the\n",
    "            yaml.\n",
    "\n",
    "        Returns:\n",
    "        - transformed_input_df: DataFrame with columns corresponding to the\n",
    "            input tensor keys in the saved model SignatureDef. The contents\n",
    "            of the columns must match the corresponding shape of the input\n",
    "            tensor described in the SignatureDef. For instance, if the\n",
    "            input to the model is a serialized tf.Example then the returned\n",
    "            DataFrame would have a single column containing serialized\n",
    "            examples.\n",
    "\n",
    "        \"\"\"\n",
    "        return pd.DataFrame({'input_1': input_df.values.tolist()})\n",
    "\n",
    "    def generate_baseline(self, input_df):\n",
    "        \"\"\"\n",
    "        Generates a DataFrame specifying a baseline that is required for\n",
    "        calculating Integrated Gradients.\n",
    "\n",
    "        The Baseline is a certain 'informationless' input relative to which\n",
    "        attributions must be computed. For instance, in a text\n",
    "        classification model, the baseline could be the empty text.\n",
    "\n",
    "        The baseline could be the same for all inputs or could be specific\n",
    "        to the input at hand. \n",
    "\n",
    "        The choice of baseline is important as explanations are contextual to a\n",
    "        baseline. For more information please refer to the following document:\n",
    "        https://github.com/ankurtaly/Integrated-Gradients/blob/master/howto.md\n",
    "        \"\"\"\n",
    "        baseline = input_df * 0\n",
    "        return pd.DataFrame({'input_1': baseline.values.tolist()})\n",
    "\n",
    "    def project_attributions(self, input_df, transformed_input_df,\n",
    "                             attributions):\n",
    "        \"\"\"\n",
    "        Maps the attributions for the provided transformed_input to\n",
    "        the original untransformed input.\n",
    "\n",
    "        This method returns a dictionary mapping features of the untransformed\n",
    "        input to the untransformed feature value, and (projected) attributions\n",
    "        computed for that feature.\n",
    "\n",
    "        This method guarantees that for each feature the projected attributions\n",
    "        have the same shape as the (returned) untransformed feature value. The\n",
    "        specific projection being applied is left as an implementation detail.\n",
    "        Below we provided some guidance on the projections that should be\n",
    "        applied for three different transformations\n",
    "\n",
    "        Identity transformation\n",
    "        This is the simplest case. Since the transformation is identity, the\n",
    "        projection would also be the identity function.\n",
    "\n",
    "        One-hot transformation for categorical features\n",
    "        Here the original feature is categorical, and the transformed feature\n",
    "        is a one-hot encoding. In this case, the returned untransformed feature\n",
    "        value is the specific input category, and the projected attribution is\n",
    "        the sum of the attribution across all fields of the one-hot encoding.\n",
    "\n",
    "        Token ID transformation for text features\n",
    "        Here the original feature is a sentence, and transformed feature is a\n",
    "        vector of token ids (w.r.t.a certain vocabulary). Here the\n",
    "        untransformed feature value would be a vector of tokens corresponding\n",
    "        to the token ids, and the projected attribution vector would be the\n",
    "        same as the one provided to this method. In some cases, token ids\n",
    "        corresponding to dummy token such a padding tokens, start tokens, end\n",
    "        tokens, etc. may be ignored during the projection. In that case, the\n",
    "        attributions values  corresponding to these tokens must be dropped from\n",
    "        the projected attributions vector.\n",
    "\n",
    "        :param input_df: Pandas DataFrame specifying the input whose prediction\n",
    "            is being attributed. Its columns must correspond to the dataset\n",
    "            yaml associated with the project. Specifically, the columns must\n",
    "            correspond to the feature names mentioned in the yaml.\n",
    "\n",
    "        :param transformed_input_df: Pandas DataFrame returned by the\n",
    "            transform_input method extended in package.py. It has exactly\n",
    "            one row as currently only instance explanations are supported.\n",
    "\n",
    "        :param attributions: dictionary mapping each column of the\n",
    "            transformed_input to the corresponding attributions tensor. The\n",
    "            attribution tensor must have the same shape as corresponding\n",
    "            column in transformed_input.\n",
    "\n",
    "        Returns:\n",
    "        - projected_inputs: dictionary with keys being the features of the\n",
    "            original untransformed input. The features are specified in the\n",
    "            model.yaml. The keys are mapped to a pair containing the original\n",
    "            untransformed input and the projected attribution.\n",
    "        \"\"\"\n",
    "        return {col: attributions['input_1'][0][i].tolist()\n",
    "                for i, col in enumerate(list(input_df.columns))}\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    model = MyModel(\n",
    "        SAVED_MODEL_PATH,\n",
    "        tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY,\n",
    "        is_binary_classification=True,\n",
    "        batch_size=32,\n",
    "        output_columns=['predicted_target'],\n",
    "        input_tensor_to_differentiable_layer_mapping=\n",
    "        {'input_1': 'serving_default_input_1:0'},\n",
    "        max_allowed_error=5\n",
    "    )\n",
    "    model.load_model()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. For Keras .h5 models\n",
    "To reiterate the point made above, Keras models have to have their input tensor (model.input) differentiable if Integrated Gradients support is desired. If that's not the case, please save the model as a TF Saved Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile heart_disease_keras/package.py\n",
    "\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "\n",
    "class MyModel:\n",
    "    def __init__(self, max_allowed_error=None,\n",
    "                 output_columns=['predicted_target']):\n",
    "        self.max_allowed_error = max_allowed_error\n",
    "\n",
    "        model_dir = pathlib.Path(__file__).parent\n",
    "\n",
    "        self.sess = tf.Session()\n",
    "        with self.sess.as_default():\n",
    "            self.model = load_model(pathlib.Path(model_dir) /\n",
    "                                    'model.h5')\n",
    "        self.ig_enabled = True\n",
    "        self.is_input_differentiable = True\n",
    "        self.batch_size = 32\n",
    "        self.output_columns = output_columns\n",
    "        self.input_tensors = self.model.input\n",
    "        self.output_tensor = self.model.output\n",
    "        self.gradient_tensors = \\\n",
    "            {self.output_columns[0]:\n",
    "                 {self.input_tensors:\n",
    "                      tf.gradients(self.output_tensor, self.input_tensors)}}\n",
    "        self.input_tensor_to_differentiable_layer_mapping = {\n",
    "            self.input_tensors: self.input_tensors}\n",
    "        self.differentiable_tensors = {self.input_tensors: self.input_tensors}\n",
    "\n",
    "    def get_feed_dict(self, input_df):\n",
    "        \"\"\"\n",
    "        Returns the input dictionary to be fed to the TensorFlow graph given\n",
    "        input_df which is a pandas DataFrame. The input_df DataFrame is\n",
    "        obtained after applying transform_input on the raw input. The\n",
    "        transform_input function is extended in package.py.\n",
    "        \"\"\"\n",
    "\n",
    "        feed = {self.input_tensors: input_df.values}\n",
    "        return feed\n",
    "\n",
    "    def transform_input(self, input_df):\n",
    "        return input_df\n",
    "\n",
    "    def generate_baseline(self, input_df):\n",
    "        return input_df*0\n",
    "\n",
    "    def predict(self, input_df):\n",
    "        transformed_input_df = self.transform_input(input_df)\n",
    "        predictions = []\n",
    "        for ind in range(0, len(transformed_input_df), self.batch_size):\n",
    "            df_chunk = transformed_input_df.iloc[ind: ind + self.batch_size]\n",
    "            feed = self.get_feed_dict(df_chunk)\n",
    "            with self.sess.as_default():\n",
    "                predictions += self.sess.run(self.output_tensor, feed).tolist()\n",
    "        return pd.DataFrame(predictions, columns=self.output_columns)\n",
    "\n",
    "    def project_attributions(self, input_df, transformed_input_df,\n",
    "                             attributions):\n",
    "        return {col: attributions[self.input_tensors][0][i].tolist()\n",
    "                for i, col in enumerate(input_df.columns)}\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    model = MyModel(max_allowed_error=1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate model package\n",
    " \n",
    "This step finds issues with the package.py composed above to enable easy debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate Keras model package\n",
    "from fiddler import PackageValidator\n",
    "validator = PackageValidator(model_info, df_schema, model_dir_keras)\n",
    "passed, errors = validator.run_chain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the parts that we need, we can go ahead and upload the model to the Fiddler platform. You can use the upload_model_custom to upload this entire directory in one shot. We need the following for uploading a model:\n",
    "- The path to the directory\n",
    "- The modelinfo that we created above, which is essentially the model schema\n",
    "- The project to which the model belongs\n",
    "- The model ID, which is the name you want to give the model. You can access it in Fiddler henceforth via this ID\n",
    "- The dataset which the model is linked to (optional)  \n",
    "\n",
    "Note: this step and all the fllowing are exactly the same for a Keras .h5 model. For our demo, we are going to upload and run the TF Saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first delete the model if it already exists in the project\n",
    "if model_id_tf in client.list_models(project_id):\n",
    "    client.delete_model(project_id, model_id_tf)\n",
    "    print('Model deleted')\n",
    "    \n",
    "client.upload_model_custom(model_dir_tf, model_info, project_id, model_id_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_input = train_input[:10]\n",
    "result = client.run_model(project_id, model_id_tf, prediction_input, log_events=True)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_point = df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_ig = client.run_explanation(\n",
    "    project_id=project_id,\n",
    "    model_id=model_id_tf, \n",
    "    df=selected_point, \n",
    "    dataset_id='heart_disease',\n",
    "    explanations='ig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "num_features = selected_point.shape[1] - 1\n",
    "sorted_att_list = sorted(list(zip(np.abs(ex_ig.attributions), ex_ig.inputs, ex_ig.attributions)),\n",
    "                         reverse=True)\n",
    "out_list = [[f[1], f[2]] for f in sorted_att_list]\n",
    "out_list = np.asarray(out_list[::-1])\n",
    "\n",
    "plt.barh(list(range(num_features)), out_list[:,1].astype('float'))\n",
    "plt.yticks(list(range(num_features)), out_list[:,0]);\n",
    "plt.xlabel('Attribution')\n",
    "plt.title(f'Top IG attributions for heart disease model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
