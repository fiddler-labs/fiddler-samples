{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import fiddler as fdl\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the nicer plotting styles from seaborn\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "This notebook assumes you already have the models and data you're interested in using uploaded to Fiddler. Please refer to the previous notebook in this series for more information on uploading to Fiddler. You will also need to have run notebook 1 in order to upload the bikesharing example used in this notebook.\n",
    "\n",
    "In this notebook we run through a number of other Fiddler functionalities that have been integrated into the Python package. Unlike the previous notebook, there is not as much of a sequential flow to the steps demonstrated here.\n",
    "\n",
    "## Before you start: set up your API connection\n",
    "\n",
    "### Launch onebox or authenticate with a remote server\n",
    "Before you can start working with a Fiddler-integrated Jupyter environment, you should set up access to a running instance of Fiddler.\n",
    "\n",
    "#### Onebox\n",
    "In onebox, this means running the `start.sh` script to launch onebox locally.\n",
    "\n",
    "#### Cloud\n",
    "For the cloud version of our product, this means looking up your authentication token in the [Fiddler settings dashboard](https://app.fiddler.ai/settings/credentials)\n",
    "\n",
    "### Create a FiddlerApi object\n",
    "\n",
    "In order to get your data and models into the Fiddler Engine, you'll need to connect using the API. The `FiddlerApi` object to handles most of the nitty-gritty for you, so all you have to do is specify some details about the Fiddler system you're connecting to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = fdl.FiddlerApi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulling data from Fiddler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see which datasets we have on Fiddler\n",
    "client.list_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset_id = 'bike_share'\n",
    "\n",
    "# client.delete_dataset(dataset_id)\n",
    "if dataset_id not in client.list_datasets():\n",
    "    df = pd.read_csv('/app/fiddler_samples/samples/datasets/bike_share/bike_share_train.csv')\n",
    "    df_schema = fdl.DatasetInfo.from_dataframe(df, max_inferred_cardinality=1000)\n",
    "    train, test = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
    "    upload_result = client.upload_dataset(\n",
    "        dataset={'train': train, 'test': test}, \n",
    "        dataset_id=dataset_id,\n",
    "        info=df_schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the info for any dataset can quickly and easily be fetched with the `dataset_info` method\n",
    "bikeshare_dataset_info = client.get_dataset_info(dataset_id)\n",
    "bikeshare_dataset_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also pull data from the dataset directly into Pandas\n",
    "bikeshare_dataset = client.get_dataset(dataset_id, max_rows=999_999)\n",
    "print(f'The bikeshare_dataset object is a {type(bikeshare_dataset)} with keys ({list(bikeshare_dataset.keys())})')\n",
    "\n",
    "df_train = bikeshare_dataset['train']\n",
    "df_test = bikeshare_dataset['test']\n",
    "\n",
    "# demo the data\n",
    "df_train.sample(3, random_state=0).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example, let's plot a regression plot of the target (cnt) against the temperature feature (temp)\n",
    "sns.regplot(df_train['temp'], df_train['cnt'], marker='.', scatter_kws=dict(alpha=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Fiddler model-builder feature\n",
    "If you have data but haven't built a model yet, you can take advantage of the model-builder feature to whip up a model instantly so you can dive right into running explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikeshare_dataset_info.get_column_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fiddler_api.delete_model('bikeshare_forecasting', 'generated_bikeshare_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: to avoid training on the whole dataset, we pass `train_splits`\n",
    "project_id = 'tutorial'\n",
    "model_id='bikeshare_model'\n",
    "client.delete_model(project_id, model_id)\n",
    "features = list(set(bikeshare_dataset_info.get_column_names()) - {'casual', 'registered', 'cnt', 'dteday'})\n",
    "client.create_model(project_id=project_id, \n",
    "                         dataset_id=dataset_id,\n",
    "                         target='cnt',\n",
    "                         features=features,\n",
    "                         model_id=model_id,\n",
    "                         train_splits=['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the new model shows up when we list the models in the bikeshare_forecasting project\n",
    "client.list_models(project_id=project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanations in Jupyter\n",
    "We also support basic integration of our explanation and prediction functionality in Jupyter. The `FiddlerApi` object is your friend here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running some predictions on the generated model\n",
    "client.run_model(project_id=project_id, model_id=model_id, df=df_test.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare against predictions on our kNN model\n",
    "client.run_model(project_id='bikeshare_forecasting', model_id='knn_model', df=df_test.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run explanations on both models\n",
    "selected_point = df_test.head(1)\n",
    "ex_generated = client.run_explanation(\n",
    "    project_id=project_id,\n",
    "    model_id=model_id, \n",
    "    df=selected_point, \n",
    "    dataset_id=dataset_id)\n",
    "\n",
    "ex_knn = client.run_explanation(\n",
    "    project_id=project_id,\n",
    "    model_id=model_id, \n",
    "    df=selected_point, \n",
    "    dataset_id=dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plot comparing attributions\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "comparison_table = pd.DataFrame({\n",
    "    'Generated Model': pd.Series(ex_generated.attributions, index=ex_generated.inputs),\n",
    "    'kNN Model': pd.Series(ex_knn.attributions, index=ex_knn.inputs)\n",
    "})\n",
    "comparison_table = comparison_table.loc[comparison_table['kNN Model'].abs().sort_values(ascending=False).index, :]\n",
    "\n",
    "melted_table = (comparison_table\n",
    "                .reset_index()\n",
    "                .rename(columns={'index': 'Feature'})\n",
    "                .melt(id_vars='Feature', \n",
    "                      var_name='Model', \n",
    "                      value_name='Attribution'))\n",
    "sns.barplot(x='Attribution', y='Feature', hue='Model', data=melted_table)\n",
    "\n",
    "plt.title('Top SHAP attributions on first row of bikeshare for generated model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "As we have seen in this notebook, once data and models have been deployed to Fiddler, it becomes very easy to share the data, automatically train a model on Fiddler, and run explanations all without leaving"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
