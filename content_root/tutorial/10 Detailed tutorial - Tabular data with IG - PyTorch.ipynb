{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Fiddler Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This python client is a powerful way to:\n",
    "- Upload the dataset and model to Fiddler\n",
    "- Ingest production events to Fiddler\n",
    "\n",
    "This can be done from a Jupyter Notebook or any python editor that you use to load data and build models.\n",
    "\n",
    "<img src=\"images/fiddler_client.png\" width=600 height=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to initialize the client object by specifying:\n",
    "- The url: url is the fiddler URL that you have been provided to access. Usually of the form ‘XXXXX.fiddler.ai’. Contact Fiddler if you don’t have it\n",
    "- The org_id: organization id is an identifier for the account. See Fiddler_URL/settings/general to find this id (listed as \"Organization ID\")\n",
    "<img src=\"images/org_id.png\" width=800 height=800 />\n",
    "- The auth_token: this token is used to authenticate access. See Fiddler_URL/settings/credentials to find, create, or change this token\n",
    "<img src=\"images/auth_token.png\" width=800 height=800 />\n",
    "\n",
    "You can also save this config as a file called fiddler.ini in the same folder as the notebook/script. That saves you from specifying the parameters in every notebook and script.\n",
    "<img src=\"images/fiddler_ini.png\" width=800 height=800 />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiddler as fdl\n",
    "\n",
    "url = 'http://trial.fiddler.ai'\n",
    "token = '7XK0ZrzVPi7eY52LBtxDlE8-9Uv8eOIRyzMKxTHhncc'\n",
    "org_id = 'anthem'\n",
    "\n",
    "client = fdl.FiddlerApi(url=url, org_id=org_id, auth_token=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fiddler has three primary constructs, namely projects, datasets and models. This diagram illustrates the relationship between the three.\n",
    "<img src=\"images/projects_data_models.png\" width=600 height=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Fiddler client provides a number of methods.\n",
    "- List datasets: ```client.list_datasets()``` List the ids of all datasets in the org.\n",
    "- List projects: ```client.list_projects()``` List the ids of all projects in the org.\n",
    "- List models: ```client.list_models()``` List the names of all models in a project.\n",
    "- Create project: ```client.create_project()``` Create a new project.\n",
    "- Create model: ```client.create_model()``` Trigger auto-modeling on a dataset already uploaded to Fiddler.\n",
    "- Get dataset info: ```client.get_dataset_info()``` Get DatasetInfo for a dataset.\n",
    "- Get model info: ```client.get_model_info()``` Get ModelInfo for a model in a certain project.\n",
    "- Get dataset: ```client.get_dataset()``` Fetches data from a dataset on Fiddler.\n",
    "- Get slice: ```client.get_slice()``` Fetches data from Fiddler via a slice query (SQL query).\n",
    "- Delete dataset: ```client.delete_dataset()``` Permanently delete a dataset.\n",
    "- Delete model: ```client.delete_model()``` Permanently delete a model.\n",
    "- Delete model artifacts: ```client.delete_model_artifacts()``` Permanently delete a model artifacts.\n",
    "- Delete project: ```client.delete_project()``` Permanently delete a project.\n",
    "- Upload dataset: ```client.upload_dataset()``` Uploads a dataset to the Fiddler engine.\n",
    "- Upload dataset from a directory: ```client.upload_dataset_from_dir()``` Uploads a dataset from a directory to the Fiddler engine.\n",
    "- Run model: ```client.run_model()``` Executes a model in the Fiddler engine on a DataFrame.\n",
    "- Run explanation: ```client.run_explanation()``` Explains a model's prediction on a single instance.\n",
    "- Run feature importance: ```client.run_feature_importance()``` Get global feature importance for a model over a dataset.\n",
    "- Upload model sklearn: ```client.upload_model_sklearn()``` Uploads a subclass of sklearn.base.BaseEstimator to the Fiddler engine.\n",
    "- Upload model package: ```client.upload_model_package()``` Uploads a custom model object to the Fiddler engine along with custom glue-code for running the model.\n",
    "- Publish event: ```client.publish_event()``` Publishes an event to Fiddler Service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data you are going to use for training your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/app/fiddler_samples/samples/datasets/heart_disease/data.csv')\n",
    "df_schema = fdl.DatasetInfo.from_dataframe(df, max_inferred_cardinality=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To upload a model, you first need to upload a sample of the data of the model’s inputs, targets, and additional metadata that might be useful for model analysis. This data sample helps us (among other things) to infer the model schema and the data types and values range of each feature.\n",
    "- This sample has to be a flat table that can be loaded as a pandas DF (```upload_dataset()```) or saved as a csv (```upload_dataset_from_dir()```).\n",
    "- In this example age, sex, trestbps, chol, fbs, thalach, exang, oldpeak, slope are input features, and target is the target column for the model.\n",
    "- This input data sample is used for many downstream functions in Fiddler\n",
    "    - Shapley value methods - background data to simulate the missing of features\n",
    "    - What-if (ICE) plots - background data\n",
    "    - PDP plots - background data\n",
    "    - Drift - to serve as a baseline\n",
    "    - Outliers - to serve as a baseline\n",
    "    - Data integrity - to serve as a baseline\n",
    "- We suggest uploading a sample of the model’s training data as it’s the most meaningful for the tasks listed above. For example, model outliers should be ideally based on the training data as that’s the data the model has seen. \n",
    "- You can upload multiple datasets with string identifiers, but we currently do not ascribe any meaning to those. For example: ```dataset={'data': df}``` or ```dataset={'train': train_df, 'test': test_df}```.\n",
    "- Currently we support two input types:\n",
    "    - Tabular\n",
    "    - Single string text, meaning text data in a single column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'heart_disease' not in client.list_datasets():\n",
    "    upload_result = client.upload_dataset(\n",
    "        dataset={'data': df}, \n",
    "        dataset_id='heart_disease')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To organize our models, let's first create a project on Fiddler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = 'pytorch_tutorial'\n",
    "\n",
    "if project_id not in client.list_projects():\n",
    "    client.create_project(project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you must have noted, in the dataset upload step we did not ask for the model’s features and targets, or any model specific information. That’s because we allow for linking multiple models to a given dataset schema. Hence we require an Infer model schema step which helps us know the features relevant to the model and the model task. Here you can specify the input features, the target column, decision columns and metadata columns, and also the type of model.\n",
    "- Currently we support only one target column. This is not to be confused with output columns, which can be more than one. \n",
    "- Decision columns specify the decisions made on the basis of the model’s predictions. For example, in a credit lending scenario, the business decision to give or not to give a loan based on the model’s output. This is helpful while monitoring models after deployment, to keep track of the business impact of the model.\n",
    "- Metadata is data that is not used by the model, but can be relevant for understanding the model’s behavior on different segments of the data. For example, gender, race, age and other such sensitive features may not be used in the model, but we can analyze along these dimensions post facto to understand if the model is biased.\n",
    "- We can infer the model task from the target column, or it can explicitly set. Currently we support three model types:\n",
    "    - Regression\n",
    "    - Binary Classification\n",
    "    - Multi-class Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'target'\n",
    "train_input = df.drop(columns=['target'])\n",
    "train_target = df[target]\n",
    "\n",
    "feature_columns = list(train_input.columns)\n",
    "\n",
    "model_info = fdl.ModelInfo.from_dataset_info(\n",
    "    dataset_info=client.get_dataset_info('heart_disease'),\n",
    "    target=target, \n",
    "    features=feature_columns,\n",
    "    display_name='PyTorch Tabular IG',\n",
    "    description='This is a PyTorch model using tabular data and IG enabled from tutorial',\n",
    "    model_task=fdl.ModelTask.BINARY_CLASSIFICATION\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install PyTorch if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, we support Sklearn version 0.21.2 and Pytorch version 1.x  \n",
    "If you have another version, please contact Fiddler for assistance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and train your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/jcjohnson/pytorch-examples\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "    \n",
    "class TwoLayerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, D_in, H1, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(TwoLayerModel, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H1)\n",
    "        self.output = torch.nn.Linear(H1, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary (differentiable) operations on Tensors.\n",
    "        \"\"\"\n",
    "        #l1 = F.relu(self.linear1(x))\n",
    "        l1 = self.linear1(x)\n",
    "        output = F.sigmoid(self.output(l1))\n",
    "        return output\n",
    "    \n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y, features, target):\n",
    "        # TODO\n",
    "        # 1. Initialize file paths or a list of file names. \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # TODO\n",
    "        # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open).\n",
    "        # 2. Preprocess the data (e.g. torchvision.Transform).\n",
    "        # 3. Return a data pair (e.g. image and label).\n",
    "        X_ind =  self.X.iloc[index:index+1]\n",
    "        return torch.tensor(X_ind[self.features].values, dtype=torch.float), \\\n",
    "               torch.tensor(self.y.iloc[index: index+1].values, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        # You should change 0 to the total size of your dataset.\n",
    "        return len(self.X)\n",
    "\n",
    "# You can then use the prebuilt data loader. \n",
    "custom_dataset_train = CustomDataset(X=train_input, y=train_target, features=feature_columns, target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-4\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "VALID_BATCH_SIZE = 32\n",
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(custom_dataset_train, **train_params)\n",
    "\n",
    "#model = FiveLayerModel(D_in=103, H1=1024, H2=512, H3=256, H4=128, H5=64, D_out=2)\n",
    "model = TwoLayerModel(D_in=9, H1=2, D_out=2)\n",
    "\n",
    "model.train()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
    "count = 0\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    \n",
    "    rounded_preds = torch.round(preds)\n",
    "    correct = (rounded_preds.argmax(axis=1) == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "def print_accuracy():\n",
    "    train_preds = model(torch.from_numpy(train_input.values).float())\n",
    "    train_y = torch.from_numpy(train_target.values).float()\n",
    "\n",
    "    print(f'Train accuracy {binary_accuracy(train_preds, train_y)}')\n",
    "\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print_accuracy()\n",
    "    for X, y in training_loader:\n",
    "        X = torch.squeeze(X)\n",
    "        y = torch.squeeze(y)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X).squeeze()\n",
    "        loss = loss_function(output, y)\n",
    "        if count % 500==0:\n",
    "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "        count += 1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "print_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model and schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step, we need to save the model and any pre-processing step you had on the input features (for example Categorical encoder, Tokenization, ...).  \n",
    "We currently support the following stored model formats:\n",
    "- For sklearn API based models, pickled models, or any storage format that you can load in the package.py (details below).\n",
    "- For TF, we support TF Saved Model and Keras .h5   \n",
    "- For PyTorch, we support any model format you can load in the package.py\n",
    "\n",
    "Note:\n",
    "- Keras models have to have their input tensor differentiable if Integrated Gradients support is desired\n",
    "- We also need to save the data preprocessing pipeline code, if any. This will be accessed in the package.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import shutil\n",
    "import yaml\n",
    "\n",
    "# Let's save the model\n",
    "model_id = 'heart_disease_pytorch'\n",
    "\n",
    "# create temp dir\n",
    "model_dir = pathlib.Path(model_id)\n",
    "shutil.rmtree(model_dir, ignore_errors=True)\n",
    "model_dir.mkdir()\n",
    "\n",
    "# save model\n",
    "torch.save(model.state_dict(), f'{model_dir}/heart_disease.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following section, we are providing the code to upload your model in the two saved format supported. Please refer to the appropriate section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write package.py and related wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Package.py is the interface between Fiddler’s backend and your model. This code helps Fiddler to understand the model, its inputs and outputs.\n",
    "- Load the model, and any associated files such as feature transformers or tokenizers.\n",
    "- Transform the data into a format that the model recognizes.\n",
    "- Make batch predictions using the model.\n",
    "- Understand the differentiable tensors of the model, in case we want to enable Integrated Gradients.\n",
    "\n",
    "For certain common highly standardized frameworks, the Fiddler client provides helper upload methods to auto-generate this module (e.g. for scikit-learn models use ```upload_model_sklearn()```).\n",
    "\n",
    "\n",
    "Writting the package.py file:\n",
    "- package.py will be invoked within the model’s specific assets directory and must implement a get_model() function which takes no arguments and returns an instance of a model class implementing the following methods:\n",
    "    - The initialization parameters For PyTorch models:\n",
    "        - ```self.max_allowed_error```: Float specifying a percentage value for the maximum allowed integral approximation error for IG computation. If None then IG will be  calculated for a pre-determined number of steps. Otherwise, the number of steps will be increased till the error is within the specified limit.\n",
    "        - ```self.model```: the code to load the model in the given session, you need to specify the file name essentially.\n",
    "        - ```self.output_columns```: a list of names of the output columns for the model.\n",
    "        - ```self.batch_size```: set a batch size for the model which will not cause OOM errors on the machines(s) the Fiddler cluster is hosted on. For the machine’s configuration, please check with Fiddler.\n",
    "        - ```self.ig_enabled```: if you want the Integrated gradients explanation method for your model. If False, then you can skip all the below parameters\n",
    "    - ```transform_input(input_df)```: Accepts a pandas DataFrame object containing rows of raw feature vectors. The output of this method can be any Python object. This function can also be used to deserialize complex data types stored in dataset columns (e.g. images stored in a field in UTF-8 format). This function is typically called by predict, but the platform may also need to invoke it directly for certain operations (e.g. computing path integral steps in the Integrated Gradients explanation method).\n",
    "    - ```generate_baseline(input_df)```: Generates a DataFrame specifying a baseline that is required for\n",
    "    calculating Integrated Gradients. The Baseline is a certain 'informationless' input relative to which\n",
    "    attributions must be computed. For instance, in a text classification model, the baseline could be the empty         text.mThe baseline could be the same for all inputs or could be specific to the input at hand. \n",
    "    The choice of baseline is important as explanations are contextual to a baseline. For more information please         refer to the following document:\n",
    "    https://github.com/ankurtaly/Integrated-Gradients/blob/master/howto.md\n",
    "    - ```predict(input_df)```: Accepts a pandas DataFrame object containing rows of raw feature vectors. Outputs a pandas DataFrame object containing the model predictions whose column labels must match the output column names in model.yaml. Typically this function invokes transform_input explicitly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate model package\n",
    " \n",
    "This step finds issues with the package.py composed above to enable easy debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile heart_disease_pytorch/package.py\n",
    "\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "PACKAGE_PATH = pathlib.Path(__file__).parent\n",
    "\n",
    "\n",
    "class TwoLayerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, D_in, H1, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(TwoLayerModel, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H1)\n",
    "        self.output = torch.nn.Linear(H1, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary (differentiable) operations on Tensors.\n",
    "        \"\"\"\n",
    "        #l1 = F.relu(self.linear1(x))\n",
    "        l1 = self.linear1(x)\n",
    "        output = F.sigmoid(self.output(l1))\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "class MyModel:\n",
    "\n",
    "\n",
    "    def __init__(self, max_allowed_error=None):\n",
    "\n",
    "\n",
    "        # Tells us it is a PyTorch model for which ig is enabled\n",
    "\n",
    "        self.ig_enabled = True\n",
    "\n",
    "        self.model_framework = 'Pytorch'\n",
    "\n",
    "\n",
    "        # Load the model to device\n",
    "\n",
    "        self.device = torch.device('cuda:4' if torch.cuda.is_available()\n",
    "\n",
    "                                   else 'cpu')\n",
    "\n",
    "\n",
    "        # Modify these lines\n",
    "\n",
    "        # -------------- Required User Input Starts  --------------------------\n",
    "\n",
    "\n",
    "        # max allowed percentage error, override to increase or decrease\n",
    "\n",
    "        # accuracy. Higher accuracy comes at a time cost\n",
    "\n",
    "        self.max_allowed_error = max_allowed_error\n",
    "\n",
    "\n",
    "        # Load the saved model\n",
    "        self.model = TwoLayerModel(D_in=9, H1=2, D_out=2)\n",
    "        self.model.load_state_dict(torch.load(PACKAGE_PATH/'heart_disease.pt', map_location=self.device))\n",
    "\n",
    "\n",
    "        # the output column names of the model, as specified in the YAML\n",
    "\n",
    "        self.output_columns = ['probability_target_True']\n",
    "\n",
    "\n",
    "        # The layer in the model to attribute. \n",
    "\n",
    "        self.layer_to_attribute = self.model.linear1\n",
    "\n",
    "\n",
    "        # If we want to attribute to the layer input\n",
    "\n",
    "        self.attribute_to_layer_input = True\n",
    "\n",
    "\n",
    "        # if we want to attribute to a particular index. For multi-class,\n",
    "\n",
    "        # we will set it to None if we want to attribute to the arg-max output\n",
    "\n",
    "        # Setting to 1 here to always attribute to the toxic class\n",
    "\n",
    "        self.target_index = 1\n",
    "\n",
    "\n",
    "        # ----------- Required User Input Ends --------------------------------\n",
    "\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        self.model.to(self.device)\n",
    "\n",
    "\n",
    "    # -------------------- User Defined Functions Start  ---------------------\n",
    "\n",
    "    def transform_input(self, input_df):\n",
    "        \"\"\"\n",
    "        Transforms the provided dataframe into a dictionary mapping the keys\n",
    "\n",
    "        'inputs' and 'auxiliary_inputs' to their corresponding tensors.\n",
    "\n",
    "\n",
    "        'inputs': Are the tensors that correspond to the\n",
    "\n",
    "                layers for which layer integrated gradients are computed. If\n",
    "\n",
    "                the model's forward_func takes a single tensor as input,\n",
    "\n",
    "                a single input tensor should be provided. If forward_func\n",
    "\n",
    "                takes multiple tensors as input, a tuple of the input tensors\n",
    "\n",
    "                should be provided.\n",
    "\n",
    "\n",
    "        'auxiliary_inputs': If the forward function requires additional\n",
    "\n",
    "                arguments other than the inputs for which attributions\n",
    "\n",
    "                should not be computed, this argument can be provided. It\n",
    "\n",
    "                must be either a single additional argument of a Tensor or\n",
    "\n",
    "                arbitrary (non-tuple) type or a tuple containing multiple\n",
    "\n",
    "                additional arguments including tensors or any arbitrary\n",
    "\n",
    "                python types.\n",
    "\n",
    "        \"\"\"\n",
    "        return {'inputs': torch.tensor(input_df.values.tolist())}\n",
    "\n",
    "    def generate_baseline(self, input_df):\n",
    "        \"\"\"\n",
    "        Creates the baseline for Integrated Gradients attributions\n",
    "\n",
    "        from the provided dataframe into a dictionary mapping the keys\n",
    "\n",
    "        'inputs' and 'auxiliary_inputs' to their corresponding tensors.\n",
    "\n",
    "\n",
    "        'inputs': Are the tensors that correspond to the\n",
    "\n",
    "                layers for which layer integrated gradients are computed. If\n",
    "\n",
    "                the model's forward_func takes a single tensor as input,\n",
    "\n",
    "                a single input tensor should be provided. If forward_func\n",
    "\n",
    "                takes multiple tensors as input, a tuple of the input tensors\n",
    "\n",
    "                should be provided.\n",
    "\n",
    "\n",
    "        'auxiliary_inputs': If the forward function requires additional\n",
    "\n",
    "                arguments other than the inputs for which attributions\n",
    "\n",
    "                should not be computed, this argument can be provided. It\n",
    "\n",
    "                must be either a single additional argument of a Tensor or\n",
    "\n",
    "                arbitrary (non-tuple) type or a tuple containing multiple\n",
    "\n",
    "                additional arguments including tensors or any arbitrary\n",
    "\n",
    "                python types.\n",
    "\n",
    "        The choice of baseline is important as explanations are contextual to a\n",
    "        baseline. For more information please refer to the following document:\n",
    "        https://github.com/ankurtaly/Integrated-Gradients/blob/master/howto.md\n",
    "        \"\"\"\n",
    "        baseline = input_df * 0\n",
    "        return {'inputs': torch.tensor(baseline.values.tolist())}\n",
    "\n",
    "    def project_attributions(self, attributions, input_df):\n",
    "        \"\"\"\n",
    "        Maps the attributions to the original input.\n",
    "\n",
    "\n",
    "        This method returns a dictionary mapping features of the untransformed\n",
    "\n",
    "        input to the untransformed feature value, and (projected) attributions\n",
    "\n",
    "        computed for that feature.\n",
    "\n",
    "\n",
    "        This method guarantees that for each feature the projected attributions\n",
    "\n",
    "        have the same shape as the (returned) untransformed feature value. The\n",
    "\n",
    "        specific projection being applied is left as an implementation detail.\n",
    "\n",
    "        Below we provided some guidance on the projections that should be\n",
    "\n",
    "        applied for three different transformations\n",
    "\n",
    "\n",
    "        Identity transformation\n",
    "\n",
    "        This is the simplest case. Since the transformation is identity, the\n",
    "\n",
    "        projection would also be the identity function.\n",
    "\n",
    "\n",
    "        One-hot transformation for categorical features\n",
    "\n",
    "        Here the original feature is categorical, and the transformed feature\n",
    "\n",
    "        is a one-hot encoding. In this case, the returned untransformed feature\n",
    "\n",
    "        value is the specific input category, and the projected attribution is\n",
    "\n",
    "        the sum of the attribution across all fields of the one-hot encoding.\n",
    "\n",
    "\n",
    "        Token ID transformation for text features\n",
    "\n",
    "        Here the original feature is a sentence, and transformed feature is a\n",
    "\n",
    "        vector of token ids (w.r.t.a certain vocabulary). Here the\n",
    "\n",
    "        untransformed feature value would be a vector of tokens corresponding\n",
    "\n",
    "        to the token ids, and the projected attribution vector would be the\n",
    "\n",
    "        same as the one provided to this method. In some cases, token ids\n",
    "\n",
    "        corresponding to dummy token such a padding tokens, start tokens, end\n",
    "\n",
    "        tokens, etc. may be ignored during the projection. In that case, the\n",
    "\n",
    "        attributions values  corresponding to these tokens must be dropped from\n",
    "\n",
    "        the projected attributions vector.\n",
    "\n",
    "\n",
    "        :param attributions: numpy array of attribution values for each of\n",
    "\n",
    "        the 'input' tensors as provided by the transform_input function\n",
    "\n",
    "        :param input_df: the original, raw input DataFrame\n",
    "\n",
    "\n",
    "        :returns: projected_inputs: dictionary with keys being the features\n",
    "\n",
    "            of the original untransformed input. The features are specified\n",
    "\n",
    "            in the model.yaml. The keys are mapped to a pair containing the\n",
    "\n",
    "            original untransformed input and the projected attribution.\n",
    "        \"\"\"\n",
    "        return {col: attributions.tolist()[i]\n",
    "                for i, col in enumerate(list(input_df.columns))}\n",
    "\n",
    "\n",
    "    # -------------------- User Provided Functions End  ---------------------\n",
    "\n",
    "\n",
    "    # -------------------- Override these if necessary  -----------------------\n",
    "\n",
    "\n",
    "    def model_function(self, *inputs):\n",
    "\n",
    "        out = self.model(*inputs)\n",
    "\n",
    "        out = torch.softmax(out, dim=-1)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def get_inputs_as_list(self, transformed_input):\n",
    "\n",
    "        list_of_inputs = []\n",
    "        list_of_inputs.append(transformed_input['inputs'])\n",
    "\n",
    "        if 'auxiliary_inputs' in transformed_input.keys():\n",
    "\n",
    "            list_of_inputs.append(transformed_input['auxiliary_inputs'])\n",
    "\n",
    "\n",
    "        return list_of_inputs\n",
    "\n",
    "\n",
    "    def predict(self, input_df):\n",
    "\n",
    "\n",
    "        transformed_input = self.transform_input(input_df)\n",
    "\n",
    "        list_of_inputs = self.get_inputs_as_list(transformed_input)\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            prediction = self.model_function(*list_of_inputs).detach().numpy()\n",
    "\n",
    "\n",
    "        if self.target_index is not None:\n",
    "\n",
    "            print(prediction.shape)\n",
    "\n",
    "            prediction = prediction[:, self.target_index]\n",
    "\n",
    "\n",
    "        return pd.DataFrame(data=prediction, columns=self.output_columns)\n",
    "\n",
    "\n",
    "\n",
    "def get_model():\n",
    "\n",
    "    model = MyModel(\n",
    "\n",
    "        max_allowed_error=99)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate Keras model package\n",
    "from fiddler import PackageValidator\n",
    "validator = PackageValidator(model_info, df_schema, model_dir)\n",
    "passed, errors = validator.run_chain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the parts that we need, we can go ahead and upload the model to the Fiddler platform. You can use the upload_model_custom to upload this entire directory in one shot. We need the following for uploading a model:\n",
    "- The path to the directory\n",
    "- The modelinfo that we created above, which is essentially the model schema\n",
    "- The project to which the model belongs\n",
    "- The model ID, which is the name you want to give the model. You can access it in Fiddler henceforth via this ID\n",
    "- The dataset which the model is linked to (optional)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first delete the model if it already exists in the project\n",
    "if model_id in client.list_models(project_id):\n",
    "    client.delete_model(project_id, model_id)\n",
    "    print('Model deleted')\n",
    "    \n",
    "client.upload_model_custom(model_dir, model_info, project_id, model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_input = train_input[:10]\n",
    "result = client.run_model(project_id, model_id, prediction_input, log_events=True)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_point = df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_ig = client.run_explanation(\n",
    "    project_id=project_id,\n",
    "    model_id=model_id, \n",
    "    df=selected_point, \n",
    "    dataset_id='heart_disease',\n",
    "    explanations='ig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "num_features = selected_point.shape[1] - 1\n",
    "sorted_att_list = sorted(list(zip(np.abs(ex_ig.attributions), ex_ig.inputs, ex_ig.attributions)),\n",
    "                         reverse=True)\n",
    "out_list = [[f[1], f[2]] for f in sorted_att_list]\n",
    "out_list = np.asarray(out_list[::-1])\n",
    "\n",
    "plt.barh(list(range(num_features)), out_list[:,1].astype('float'))\n",
    "plt.yticks(list(range(num_features)), out_list[:,0]);\n",
    "plt.xlabel('Attribution')\n",
    "plt.title(f'Top IG attributions for heart disease model')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
