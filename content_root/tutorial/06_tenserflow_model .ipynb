{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import pathlib\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "import category_encoders\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sklearn.metrics\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "\n",
    "import fiddler as fdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "In tutorial_01, we demonstrated how to quickly and easily upload a `scikit-learn` model to Fiddler. In this tutorial, we show how to upload complex models using the custom model upload API endpoint.\n",
    "\n",
    "This notebook is organized into three sections\n",
    "1. Loading the Bikeshare data, preprocessing it, and uploading it to Fiddler.\n",
    "2. Building a multilayer perceptron model using Tensorflow 1.x to predict hourly rentals inthe Bikeshare data.\n",
    "2. Uploading this MLP model to the Fiddler platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Loading data\n",
    "\n",
    "As we saw in tutorial_01, as long as your data can be dumped into a DataFrame object, there is nothing else you need to do to get it ready to upload to Fiddler. However, for an MLP model we will need to transform our dataset with proper categorical encoding and standardization. We will demonstrate uploading both the original and preprocessed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Downloading the UCI bikeshare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>instant</th>\n",
       "      <th>3440</th>\n",
       "      <th>6543</th>\n",
       "      <th>15471</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dteday</th>\n",
       "      <td>2011-05-28 00:00:00</td>\n",
       "      <td>2011-10-05 00:00:00</td>\n",
       "      <td>2012-10-11 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>season</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yr</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mnth</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>holiday</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekday</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>workingday</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weathersit</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>temp</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>atemp</th>\n",
       "      <td>0.5303</td>\n",
       "      <td>0.4394</td>\n",
       "      <td>0.4394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hum</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>windspeed</th>\n",
       "      <td>0.2239</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>casual</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>registered</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnt</th>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "instant                   3440                 6543                 15471\n",
       "dteday      2011-05-28 00:00:00  2011-10-05 00:00:00  2012-10-11 00:00:00\n",
       "season                        2                    4                    4\n",
       "yr                            0                    0                    1\n",
       "mnth                          5                   10                   10\n",
       "hr                            5                    4                   19\n",
       "holiday                   False                False                False\n",
       "weekday                       6                    3                    4\n",
       "workingday                False                 True                 True\n",
       "weathersit                    1                    1                    1\n",
       "temp                       0.56                 0.44                 0.44\n",
       "atemp                    0.5303               0.4394               0.4394\n",
       "hum                        0.88                 0.88                 0.51\n",
       "windspeed                0.2239                    0               0.1343\n",
       "casual                        4                    1                   81\n",
       "registered                    3                    4                  662\n",
       "cnt                           7                    5                  743"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set (bikeshare rentals in 2011) has 8645 rows, test set (bikeshare rentals in 2012) has 8734 rows\n"
     ]
    }
   ],
   "source": [
    "zip_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip'\n",
    "z = zipfile.ZipFile(io.BytesIO(requests.get(zip_url).content))\n",
    "\n",
    "# here we pre-configure the datatypes for our dataframe\n",
    "# so it doesn't require any datatype modification after import\n",
    "bikeshare_dtypes = dict(season='category', holiday='bool',\n",
    "                        workingday='bool', weathersit='category')\n",
    "bikeshare_datetime_columns = ['dteday']\n",
    "bikeshare_index_column = 'instant'\n",
    "with z.open('hour.csv') as csv:\n",
    "    df = pd.read_csv(csv, \n",
    "                     dtype=bikeshare_dtypes, \n",
    "                     parse_dates=bikeshare_datetime_columns,\n",
    "                     index_col=bikeshare_index_column)\n",
    "\n",
    "# split train/test by year\n",
    "is_2011 = df['yr'] == 0\n",
    "df_2011 = df[is_2011].reset_index(drop=True)\n",
    "df_2012 = df[~is_2011].reset_index(drop=True)\n",
    "\n",
    "# peek at the data\n",
    "display(df.sample(3, random_state=0).T)\n",
    "\n",
    "# print info about train-test split\n",
    "print(f'Train set (bikeshare rentals in 2011) has {df_2011.shape[0]} rows,'\n",
    "      f' test set (bikeshare rentals in 2012) has {df_2012.shape[0]} rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify which columns are features and which are not\n",
    "target = 'cnt'\n",
    "not_used_as_features = ['dteday', 'yr', 'casual', 'registered']\n",
    "non_feature_columns = [target] + not_used_as_features\n",
    "feature_columns = list(set(df_2011.columns) - set(non_feature_columns))\n",
    "\n",
    "# split our data into features and targets\n",
    "x_train = df_2011.drop(columns=non_feature_columns)\n",
    "x_test = df_2012.drop(columns=non_feature_columns)\n",
    "y_train = df_2011[target]\n",
    "y_test = df_2012[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Feature preprocessing\n",
    "As we can see above, this dataset contains some categorical features (`season` and `workingday`) as well as features on different scales (`hr` and `temp`). Since encoding categorical variables can be a pain in `sklearn`, we will use the `category_encoders` package, and combine this with the `StandardScaler` transformation from scikit-learn in a `Pipeline` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot = category_encoders.OneHotEncoder(cols=df.select_dtypes('category').columns.tolist())\n",
    "standard_scaler = sklearn.preprocessing.StandardScaler()\n",
    "preprocessor = sklearn.pipeline.make_pipeline(onehot, standard_scaler)\n",
    "preprocessor.fit(x_train)\n",
    "x_train_processed = preprocessor.transform(x_train)\n",
    "x_test_processed = preprocessor.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Uploading the data to Fiddler\n",
    "\n",
    "### Before you start: set up your API connection\n",
    "\n",
    "#### Onebox\n",
    "If you're using a Onebox deployment, make sure you've run the `start.sh` script to launch Onebox locally.\n",
    "\n",
    "#### Cloud\n",
    "For the cloud version of our product, look up your authentication token in the [Fiddler settings dashboard](https://app.fiddler.ai/settings/credentials)\n",
    "\n",
    "#### Create a FiddlerApi object\n",
    "In order to get your data and models into the Fiddler Engine, you'll need to connect using the API. The `FiddlerApi` object to handles most of the nitty-gritty for you, so all you have to do is specify some details about the Fiddler system you're connecting to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: typically the API url for your running instance of Fiddler will beÂ \"https://api.fiddler.ai\" (or \"http://localhost:4100\" for onebox)\n",
    "# however, use \"http://host.docker.internal:4100\" as our URL if Jupyter is running in a docker VM on the same macOS machine as onebox\n",
    "url = 'http://host.docker.internal:4100'\n",
    "\n",
    "# see <Fiddler URL>/settings/credentials to find, create, or change this token\n",
    "token = os.getenv('FIDDLER_API_TOKEN')\n",
    "\n",
    "# see <Fiddler URL>/settings/general to find this id (listed as \"Organization Name\")\n",
    "org_id = 'onebox'\n",
    "\n",
    "fiddler_api = fdl.FiddlerApi(url=url, org_id=org_id, auth_token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dataset deleted bikeshare_processed'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete the datasets if we've uploaded them previously\n",
    "fiddler_api.delete_dataset('bikeshare')\n",
    "fiddler_api.delete_dataset('bikeshare_processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Heads up! We are inferring the details of your dataset from the dataframe(s) provided. Please take a second to check our work.\n",
      "\n",
      "If the following DatasetInfo is an incorrect representation of your data, you can construct a DatasetInfo with the DatasetInfo.from_dataframe() method and modify that object to reflect the correct details of your dataset.\n",
      "\n",
      "After constructing a corrected DatasetInfo, please re-upload your dataset with that DatasetInfo object explicitly passed via the `info` parameter of FiddlerApi.upload_dataset().\n",
      "\n",
      "You may need to delete the initially uploaded versionvia FiddlerApi.delete_dataset('bikeshare').\n",
      "\n",
      "Inferred DatasetInfo to check:\n",
      "  DatasetInfo:\n",
      "    display_name: bikeshare\n",
      "    files: []\n",
      "    columns:\n",
      "              column     dtype count(possible_values)\n",
      "      0       dteday    STRING                      -\n",
      "      1       season  CATEGORY                      4\n",
      "      2           yr   INTEGER                      -\n",
      "      3         mnth   INTEGER                      -\n",
      "      4           hr   INTEGER                      -\n",
      "      5      holiday   BOOLEAN                      -\n",
      "      6      weekday   INTEGER                      -\n",
      "      7   workingday   BOOLEAN                      -\n",
      "      8   weathersit  CATEGORY                      4\n",
      "      9         temp     FLOAT                      -\n",
      "      10       atemp     FLOAT                      -\n",
      "      11         hum     FLOAT                      -\n",
      "      12   windspeed     FLOAT                      -\n",
      "      13      casual   INTEGER                      -\n",
      "      14  registered   INTEGER                      -\n",
      "      15         cnt   INTEGER                      -\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'row_count': 17379,\n",
       " 'col_count': 16,\n",
       " 'log': ['Importing dataset bikeshare',\n",
       "  'Creating table for bikeshare',\n",
       "  'Importing data file: test.csv',\n",
       "  'Importing data file: train.csv']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's upload the original dataset\n",
    "fiddler_api.upload_dataset(\n",
    "    dataset={'train': df_2011, 'test': df_2012}, \n",
    "    dataset_id='bikeshare')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Heads up! We are inferring the details of your dataset from the dataframe(s) provided. Please take a second to check our work.\n",
      "\n",
      "If the following DatasetInfo is an incorrect representation of your data, you can construct a DatasetInfo with the DatasetInfo.from_dataframe() method and modify that object to reflect the correct details of your dataset.\n",
      "\n",
      "After constructing a corrected DatasetInfo, please re-upload your dataset with that DatasetInfo object explicitly passed via the `info` parameter of FiddlerApi.upload_dataset().\n",
      "\n",
      "You may need to delete the initially uploaded versionvia FiddlerApi.delete_dataset('bikeshare_processed').\n",
      "\n",
      "Inferred DatasetInfo to check:\n",
      "  DatasetInfo:\n",
      "    display_name: bikeshare_processed\n",
      "    files: []\n",
      "    columns:\n",
      "                column    dtype count(possible_values)\n",
      "      0       season_1    FLOAT                      -\n",
      "      1       season_2    FLOAT                      -\n",
      "      2       season_3    FLOAT                      -\n",
      "      3       season_4    FLOAT                      -\n",
      "      4           mnth    FLOAT                      -\n",
      "      5             hr    FLOAT                      -\n",
      "      6        holiday    FLOAT                      -\n",
      "      7        weekday    FLOAT                      -\n",
      "      8     workingday    FLOAT                      -\n",
      "      9   weathersit_1    FLOAT                      -\n",
      "      10  weathersit_2    FLOAT                      -\n",
      "      11  weathersit_3    FLOAT                      -\n",
      "      12  weathersit_4    FLOAT                      -\n",
      "      13          temp    FLOAT                      -\n",
      "      14         atemp    FLOAT                      -\n",
      "      15           hum    FLOAT                      -\n",
      "      16     windspeed    FLOAT                      -\n",
      "      17           cnt  INTEGER                      -\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'row_count': 17379,\n",
       " 'col_count': 18,\n",
       " 'log': ['Importing dataset bikeshare_processed',\n",
       "  'Creating table for bikeshare_processed',\n",
       "  'Importing data file: test.csv',\n",
       "  'Importing data file: train.csv']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's also upload the preprocessed version of the dataset\n",
    "df_2011_processed = pd.concat([pd.DataFrame(x_train_processed, columns=onehot.feature_names), y_train], axis=1)\n",
    "df_2012_processed = pd.concat([pd.DataFrame(x_test_processed, columns=onehot.feature_names), y_test], axis=1)\n",
    "fiddler_api.upload_dataset(\n",
    "    dataset={'train': df_2011_processed, 'test': df_2012_processed}, \n",
    "    dataset_id='bikeshare_processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['imdb_rnn',\n",
       " 'ieee_fraud',\n",
       " 'iris',\n",
       " 'bank_churn',\n",
       " '20news',\n",
       " 'p2p_loans',\n",
       " 'bikeshare_processed',\n",
       " 'titanic',\n",
       " 'winequality',\n",
       " 'bikeshare']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we see that the 'bikeshare' and 'bikeshare_processed' shows up in the list of all datasets\n",
    "fiddler_api.list_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing the data on Fiddler\n",
    "We can also verify everything worked by looking at the web UI:\n",
    "- http://localhost:4100/datasets\n",
    "\n",
    "(or if you used cloud instead of onebox)\n",
    "- https://app.fiddler.ai/datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Building a Tensorflow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: tensorflow==1.* in /usr/local/lib/python3.6/dist-packages (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.*) (0.8.0)\n",
      "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.*) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.*) (1.16.4)\n",
      "Requirement already satisfied, skipping upgrade: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.*) (0.2.2)\n",
      "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.*) (0.1.7)\n",
      "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.*) (1.0.8)\n",
      "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.*) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.*) (1.21.1)\n",
      "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.*) (0.7.1)\n",
      "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.*) (1.11.1)\n",
      "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.*) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.*) (1.15.1)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.*) (3.8.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.*) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.*) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.*) (0.33.4)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.*) (2.9.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.*) (41.0.1)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.*) (0.15.4)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.*) (3.1.1)\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ensure TF 1 (and latest version), but not TF 2\n",
    "!pip install --upgrade tensorflow==1.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# NOTE: we can only run tf.saved_model.save() on a tf.keras model if we use eager execution\n",
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "# triple-check version is 1.x\n",
    "print(tf.__version__)\n",
    "assert tf.__version__[0] == '1', 'Stop! This tutorial is meant to use TF 1.x!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8645 samples\n",
      "Epoch 1/2\n",
      "8645/8645 [==============================] - 1s 171us/sample - loss: 12402.2006\n",
      "Epoch 2/2\n",
      "8645/8645 [==============================] - 1s 108us/sample - loss: 10403.2758\n",
      "Train on 8645 samples\n",
      "Epoch 1/8\n",
      "8645/8645 [==============================] - 1s 58us/sample - loss: 9418.3922\n",
      "Epoch 2/8\n",
      "8645/8645 [==============================] - 0s 57us/sample - loss: 7553.8729\n",
      "Epoch 3/8\n",
      "8645/8645 [==============================] - 1s 60us/sample - loss: 6095.5634\n",
      "Epoch 4/8\n",
      "8645/8645 [==============================] - 1s 58us/sample - loss: 4535.0049\n",
      "Epoch 5/8\n",
      "8645/8645 [==============================] - 1s 62us/sample - loss: 3560.8279\n",
      "Epoch 6/8\n",
      "8645/8645 [==============================] - 0s 58us/sample - loss: 2906.7335\n",
      "Epoch 7/8\n",
      "8645/8645 [==============================] - 1s 65us/sample - loss: 2621.5162\n",
      "Epoch 8/8\n",
      "8645/8645 [==============================] - 1s 59us/sample - loss: 2246.5458\n",
      "The model achieves a test-set r2 score of 0.50\n"
     ]
    }
   ],
   "source": [
    "# Train a 2-layer MLP model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.003),\n",
    "    loss='mse')\n",
    "model.fit(x_train_processed, y_train.values, batch_size=16, epochs=2)\n",
    "model.optimizer.learning_rate = 0.01\n",
    "model.fit(x_train_processed, y_train.values, batch_size=32, epochs=8)\n",
    "\n",
    "y_hat = model.predict(x_test_processed)\n",
    "r2 = sklearn.metrics.r2_score(y_test, y_hat)\n",
    "print(f'The model achieves a test-set r2 score of {r2:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Uploading TF model to Fiddler\n",
    "Now that we have trained a model, let's move on to uploding the model to Fiddler.\n",
    "\n",
    "For Tensorflow, the [SavedModel format](https://www.tensorflow.org/guide/saved_model) provides a powerful way to save and deploy models across versions and languages. Below we demonstrate a generic wrapper class that can be used to quickly and easily deploy to Fiddler any TF model.\n",
    "\n",
    "First, we save our model to the SavedModel format using `tf.saved_model.save`. Then, we save a `package.py` file that contains the glue code needed for the Fiddler platform to loand and run the SavedModel. Lastly, we'll upload the SavedModel and the `package.py` file using the `fdl.FiddlerApi.upload_model_custom()` function.\n",
    "\n",
    "### TF SavedModel and `package.py`\n",
    "Since it is a common task to run TF models saved in the SavedModel format, the Fiddler Python package provides a generic model loader class called `TFSavedModel`. Let's take a look at the source code below. We'll notice that the `__init__` method loads the model from the SavedModel format, and that it offers a single public method `.predict()` which runs the model on input provided as a pandas DataFrame. This is the standard convention for all model-runners in Fiddler. \n",
    "\n",
    "We also notice that this class takes an optional keyword argument `input_transformation` which allows the user to define a custom mapping the DataFrame input into the matrix/tensor inputs consumed by the model. Later we will use this functionality to upload our `preprocessor` object along with the model to avoid the necessity of preprocessing our dataset before uploading it to Fiddler (using the un-processed dataset also offers the benefit of explanations being computed in terms of the original dataset features, rather than the model's internal preprocessed features, which are often less human-interpretable).\n",
    "\n",
    "If the `TFSavedModel` class is not sufficient to run your TF model, it might still be helpful to either subclass it with your own custom model loader or use its source code as a starting point for writing a model loader from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mTFSavedModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msaved_model_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0moutput_column_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mis_binary_classification\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minput_transformation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0mTFSavedModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mat\u001b[0m \u001b[0;36m0x7fe14404b620\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m      <no docstring>\n",
       "\u001b[0;31mSource:\u001b[0m        \n",
       "\u001b[0;32mclass\u001b[0m \u001b[0mTFSavedModel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0msaved_model_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0moutput_column_names\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mis_binary_classification\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0minput_transformation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mlambda\u001b[0m \u001b[0minput_df\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minput_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Load and run a TF model saved in a saved_model format.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        NOTE: The model is loaded using the default serving signature def.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        Args:\u001b[0m\n",
       "\u001b[0;34m        :param saved_model_path: Path to the directory containing the TF\u001b[0m\n",
       "\u001b[0;34m            model in SavedModel format.\u001b[0m\n",
       "\u001b[0;34m            See: https://www.tensorflow.org/guide/saved_model#build_and_load_a_savedmodel  # noqa E501\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        :param output_column_names: List containing the names of the output\u001b[0m\n",
       "\u001b[0;34m            column(s) that corresponds to the output of the model. If the\u001b[0m\n",
       "\u001b[0;34m            model is a binary classification model then the number of output\u001b[0m\n",
       "\u001b[0;34m            columns must be one, even if the output tensor has two columns.\u001b[0m\n",
       "\u001b[0;34m            Otherwise, the number of columns must match the\u001b[0m\n",
       "\u001b[0;34m            shape of the output tensor corresponding to the output key\u001b[0m\n",
       "\u001b[0;34m            specified.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m         :param is_binary_classification [optional]: Boolean specifying if the\u001b[0m\n",
       "\u001b[0;34m            model is a binary classification model. If True, the number of\u001b[0m\n",
       "\u001b[0;34m            output columns is one. The default is False.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        :param batch_size [optional]: the batch size for input into the model.\u001b[0m\n",
       "\u001b[0;34m            Depends on model and instance config.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        :param input_transformation [optional]: This function transforms\u001b[0m\n",
       "\u001b[0;34m            a DataFrame of inputs into a sequence of arrays/tensors that can be\u001b[0m\n",
       "\u001b[0;34m            fed into the input tensors of the model.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m            NOTE: The default input_transformation assumed the model has a\u001b[0m\n",
       "\u001b[0;34m                single input that accepts the array df.values. A custom\u001b[0m\n",
       "\u001b[0;34m                input_transformation must be provided if a more complicated\u001b[0m\n",
       "\u001b[0;34m                transformation is needed between DataFrame and model inputs.\u001b[0m\n",
       "\u001b[0;34m            NOTE: If the model has multiple inputs, the order of the elements\u001b[0m\n",
       "\u001b[0;34m                of the sequence returned by input_transformation should match\u001b[0m\n",
       "\u001b[0;34m                the order of the inputs in the model's signature def.\u001b[0m\n",
       "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaved_model_path\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_column_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_column_names\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_binary_classification\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_binary_classification\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_transformation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_transformation\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# load the model\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0msaved_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0msess\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'serve'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mexport_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# load the signature def\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0msig_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature_def\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature_constants\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m.\u001b[0m\u001b[0mDEFAULT_SERVING_SIGNATURE_DEF_KEY\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# find model input names from the signature def\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tensor_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mfor\u001b[0m \u001b[0minp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msig_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# identify the output tensor from the signature def\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m#  using only the positive class output in binary classification\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34mf'Only single-output models are supported, but this model has '\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34mf'{len(sig_def.outputs)}.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tensor_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0msig_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mis_binary_classification\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_df\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Returns predictions for the provided inputs.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        Args:\u001b[0m\n",
       "\u001b[0;34m        :param input_df: DataFrame of input features.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        :return: DataFrame with predictions for the provided inputs.\u001b[0m\n",
       "\u001b[0;34m            The columns of the DataFrame correspond in name and order\u001b[0m\n",
       "\u001b[0;34m            to the model's output_column_names.\u001b[0m\n",
       "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0minput_values_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_transformation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mn_row\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mn_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_column_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mpred_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_row\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mbatch_slice_gen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mind\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                           \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_row\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mfor\u001b[0m \u001b[0mbatch_slice\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_slice_gen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                    \u001b[0mmodel_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minput_values_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_slice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                    \u001b[0;32mfor\u001b[0m \u001b[0mmodel_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_values_tensor\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                    \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_values_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mpred_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_slice\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_column_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m           /mnt/client/fiddler/model_loaders.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fiddler.model_loaders import TFSavedModel\n",
    "??TFSavedModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Saving the model\n",
    "We begin by creating a new directory to save our model files and `package.py` file into. Then we save the model using the Tensorflow SavedModel format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1206 20:41:56.768882 140608237893440 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "# (re-)create directory for the model\n",
    "model_dir = pathlib.Path('tf_model')\n",
    "shutil.rmtree(model_dir, ignore_errors=True)\n",
    "model_dir.mkdir()\n",
    "# save the model\n",
    "tf.saved_model.save(model, str(model_dir / 'saved_model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Making the SavedModel run in Fiddler\n",
    "In order for the model to run on Fiddler, we need to author a `package.py` file to glue things together. Let's develop the logic that fits into this file piece by piece here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1206 20:41:56.981302 140608237893440 module_wrapper.py:139] From /mnt/client/fiddler/model_loaders.py:62: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "W1206 20:41:56.983628 140608237893440 deprecation.py:323] From /mnt/client/fiddler/model_loaders.py:65: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
      "W1206 20:41:57.070469 140608237893440 module_wrapper.py:139] From /mnt/client/fiddler/model_loaders.py:69: The name tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY is deprecated. Please use tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted_bike_rentals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32.117195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   predicted_bike_rentals\n",
       "0               32.117195"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loader works!\n"
     ]
    }
   ],
   "source": [
    "# first, let's make sure we can run our model using the TFSavedModel class\n",
    "# to do this, all we need is the path to our SavedModel files and a name\n",
    "# for our model output\n",
    "model_prediction_name = 'predicted_bike_rentals'\n",
    "fiddler_model = TFSavedModel(\n",
    "    saved_model_path=model_dir / 'saved_model', \n",
    "    output_column_names=[model_prediction_name],\n",
    "    is_binary_classification=False\n",
    ")\n",
    "input_df = pd.DataFrame(x_test_processed[:1], columns=onehot.feature_names)\n",
    "pred = fiddler_model.predict(input_df)\n",
    "display(pred)\n",
    "success = pred.values == model.predict(x_test_processed[:1])\n",
    "assert success\n",
    "print('Model loader works!')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensuring model output consistency\n",
    "Above we arbitrarily chose the name 'predicted_bike_rentals' for our model output. However, as we showed in tutorial_01, when a model is uploaded to Fiddler it is accompanied by a `ModelInfo` object that specifies metatdata like the names, types, and order of model inputs and outputs. To ensure consistency, it is best practice to programatically read the output name from the `ModelInfo` rather than storing it in two places at once (i.e. `ModelInfo` *and* `package.py`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelInfo:\n",
       "  display_name:  model\n",
       "  description: None\n",
       "  input_type: ModelInputType.TABULAR\n",
       "  model_task: ModelTask.REGRESSION\n",
       "  inputs and outputs:\n",
       "                        column column_type  dtype count(possible_values)\n",
       "    0                 season_1       input  FLOAT                      -\n",
       "    1                 season_2       input  FLOAT                      -\n",
       "    2                 season_3       input  FLOAT                      -\n",
       "    3                 season_4       input  FLOAT                      -\n",
       "    4                     mnth       input  FLOAT                      -\n",
       "    5                       hr       input  FLOAT                      -\n",
       "    6                  holiday       input  FLOAT                      -\n",
       "    7                  weekday       input  FLOAT                      -\n",
       "    8               workingday       input  FLOAT                      -\n",
       "    9             weathersit_1       input  FLOAT                      -\n",
       "    10            weathersit_2       input  FLOAT                      -\n",
       "    11            weathersit_3       input  FLOAT                      -\n",
       "    12            weathersit_4       input  FLOAT                      -\n",
       "    13                    temp       input  FLOAT                      -\n",
       "    14                   atemp       input  FLOAT                      -\n",
       "    15                     hum       input  FLOAT                      -\n",
       "    16               windspeed       input  FLOAT                      -\n",
       "    17  predicted_bike_rentals      output  FLOAT                      -\n",
       "  targets: [Column(name=\"cnt\", data_type=DataType.INTEGER, possible_values=None)]\n",
       "  misc:\n",
       "    {}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's start by creating a ModelInfo for our model\n",
    "# where the model inputs are the processed data\n",
    "processed_dataset_info = fdl.DatasetInfo.from_dataframe(df_2011_processed)\n",
    "model_of_processed_features_info = fdl.ModelInfo.from_dataset_info(\n",
    "    processed_dataset_info,\n",
    "    target='cnt',\n",
    "    features=onehot.feature_names,\n",
    "    input_type=fdl.ModelInputType.TABULAR\n",
    ")\n",
    "\n",
    "# we can set the desired output name in the ModelInfo\n",
    "model_of_processed_features_info.outputs[0].name = 'predicted_bike_rentals'\n",
    "\n",
    "model_of_processed_features_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loader works!\n"
     ]
    }
   ],
   "source": [
    "# now we can pull the output names from the ModelInfo when we load the model\n",
    "output_names = model_of_processed_features_info.get_output_names()\n",
    "fiddler_model = TFSavedModel(\n",
    "    saved_model_path=model_dir / 'saved_model', \n",
    "    output_column_names=output_names,\n",
    "    is_binary_classification=False\n",
    ")\n",
    "input_df = pd.DataFrame(x_test_processed[:1], columns=onehot.feature_names)\n",
    "success = fiddler_model.predict(input_df).values == model.predict(x_test_processed[:1])\n",
    "assert success\n",
    "print('Model loader works!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading ModelInfo inside `package.py`\n",
    "When a model is uploaded to Fiddler, the ModelInfo is serialized alongsize the `package.py` file in the form of a `model.yaml` file. Here we will show you how you can write a method to load the ModelInfo within the Fiddler engine, and how to mock this process locally to ensure your `package.py` code will operate properly when running on Fiddler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ModelInfo to tf_model/model.yaml...\n",
      "tf_model/model.yaml successfully removed.\n",
      "ModelInfo properly read from disk!\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "def mock_model_yaml(model_info, model_dir):\n",
    "    \"\"\"Mock writing ModelInfo to model.yaml\n",
    "    NOTE: the ModelInfo.to_dict() dictionary should be nested inside another \n",
    "        dictionary before it is written to model.yaml\n",
    "    \"\"\"\n",
    "    yaml_file_path = pathlib.Path(model_dir) / 'model.yaml'\n",
    "    print(f'Writing ModelInfo to {str(yaml_file_path)}...')\n",
    "    with yaml_file_path.open('w') as yaml_file:\n",
    "        yaml.dump({'model': model_info.to_dict()}, yaml_file)\n",
    "        \n",
    "def un_mock_model_yaml(model_dir):\n",
    "    \"\"\"Clean up a mocked model.yaml file\"\"\"\n",
    "    model_yaml_path = pathlib.Path(model_dir) / 'model.yaml'\n",
    "    try:\n",
    "        model_yaml_path.unlink()\n",
    "        print(f'{model_yaml_path} successfully removed.')\n",
    "    except FileNotFoundError:\n",
    "        print('No model.yaml file found')\n",
    "    \n",
    "def load_model_info(model_dir):\n",
    "    \"\"\"Load ModelInfo from a model.yaml file\"\"\"\n",
    "    with (pathlib.Path(model_dir) / 'model.yaml').open('r') as yaml_file:\n",
    "        return fdl.ModelInfo.from_dict(yaml.load(yaml_file, Loader=yaml.SafeLoader))\n",
    "    \n",
    "mock_model_yaml(model_of_processed_features_info, model_dir)\n",
    "test_model_info = load_model_info(model_dir)\n",
    "un_mock_model_yaml(model_dir)\n",
    "success = test_model_info.get_output_names() == model_of_processed_features_info.get_output_names()\n",
    "assert success\n",
    "print('ModelInfo properly read from disk!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `get_model()` function\n",
    "Lastly, we need to address the convention of `package.py`, which is to provide a function `get_model()` that returns a model object which offers a DataFrame -> DataFrame `.predict()` method. Let's write and test a `get_model()` function here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, in `package.py` we can simply load this model info as follows\n",
    "def get_model(model_dir, tf_saved_model_dir):\n",
    "    model_info = load_model_info(model_dir)\n",
    "    output_names = model_info.get_output_names()\n",
    "    is_binary_classification = (\n",
    "        model_info.model_task.name \n",
    "            == fdl.ModelTask.BINARY_CLASSIFICATION.name\n",
    "    )\n",
    "    return TFSavedModel(\n",
    "        tf_saved_model_dir, \n",
    "        output_column_names=output_names,\n",
    "        is_binary_classification=is_binary_classification\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ModelInfo to tf_model/model.yaml...\n",
      "tf_model/model.yaml successfully removed.\n",
      "Model loader works!\n"
     ]
    }
   ],
   "source": [
    "# test get_model()\n",
    "mock_model_yaml(model_of_processed_features_info, model_dir)\n",
    "fiddler_model = get_model(model_dir, model_dir / 'saved_model')\n",
    "un_mock_model_yaml(model_dir)\n",
    "\n",
    "input_df = pd.DataFrame(x_test_processed[:1], columns=onehot.feature_names)\n",
    "success = fiddler_model.predict(input_df).values == model.predict(x_test_processed[:1])\n",
    "assert success\n",
    "print('Model loader works!')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing package.py\n",
    "Now all we need to do is combine our `load_model_info()` and `get_model()` functions into a script called `package.py`.\n",
    "\n",
    "In doing so, there is one tricky part: we need to make sure to specify the `model_dir` and `tf_saved_model_dir` keyword arguments in the `get_model()` function, since the Fiddler engine will not not pass any arguments when it calls this method (the arguments above are just to enable us to test the method in this notebook). What is tricky about this is that we have to programatically ascertain the file's path, since the caller's current working directory may differ from the directory in which the model is stored. To do this, we recommend the following idiom:\n",
    "\n",
    "`MODEL_DIR = pathlib.Path(__file__).parent`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "package_py_contents = '''\n",
    "import pathlib\n",
    "\n",
    "import yaml\n",
    "\n",
    "import fiddler as fdl\n",
    "from fiddler.model_loaders import TFSavedModel\n",
    "\n",
    "MODEL_DIR = pathlib.Path(__file__).parent\n",
    "\n",
    "def load_model_info(model_dir):\n",
    "    \"\"\"Load ModelInfo from a model.yaml file\"\"\"\n",
    "    with (pathlib.Path(model_dir) / 'model.yaml').open('r') as yaml_file:\n",
    "        return fdl.ModelInfo.from_dict(yaml.load(yaml_file, Loader=yaml.SafeLoader))\n",
    "\n",
    "def get_model(model_dir=MODEL_DIR, tf_saved_model_dir=MODEL_DIR / 'saved_model'):\n",
    "    model_info = load_model_info(model_dir)\n",
    "    output_names = model_info.get_output_names()\n",
    "    is_binary_classification = (\n",
    "        model_info.model_task.name \n",
    "            == fdl.ModelTask.BINARY_CLASSIFICATION.name\n",
    "    )\n",
    "    return TFSavedModel(\n",
    "        tf_saved_model_dir, \n",
    "        output_column_names=output_names,\n",
    "        is_binary_classification=is_binary_classification\n",
    "    )\n",
    "'''\n",
    "with (model_dir / 'package.py').open('w') as f:\n",
    "    f.write(package_py_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Uploading the model\n",
    "Now that we have saved our model, created a `ModelInfo`, and written a working `package.py`, we can now upload the model using the Fiddler API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project already exists, no change.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': {'name': ' model',\n",
       "  'input-type': 'structured',\n",
       "  'model-task': 'regression',\n",
       "  'inputs': [{'column-name': 'season_1', 'data-type': 'float'},\n",
       "   {'column-name': 'season_2', 'data-type': 'float'},\n",
       "   {'column-name': 'season_3', 'data-type': 'float'},\n",
       "   {'column-name': 'season_4', 'data-type': 'float'},\n",
       "   {'column-name': 'mnth', 'data-type': 'float'},\n",
       "   {'column-name': 'hr', 'data-type': 'float'},\n",
       "   {'column-name': 'holiday', 'data-type': 'float'},\n",
       "   {'column-name': 'weekday', 'data-type': 'float'},\n",
       "   {'column-name': 'workingday', 'data-type': 'float'},\n",
       "   {'column-name': 'weathersit_1', 'data-type': 'float'},\n",
       "   {'column-name': 'weathersit_2', 'data-type': 'float'},\n",
       "   {'column-name': 'weathersit_3', 'data-type': 'float'},\n",
       "   {'column-name': 'weathersit_4', 'data-type': 'float'},\n",
       "   {'column-name': 'temp', 'data-type': 'float'},\n",
       "   {'column-name': 'atemp', 'data-type': 'float'},\n",
       "   {'column-name': 'hum', 'data-type': 'float'},\n",
       "   {'column-name': 'windspeed', 'data-type': 'float'}],\n",
       "  'outputs': [{'column-name': 'predicted_bike_rentals', 'data-type': 'float'}],\n",
       "  'targets': [{'column-name': 'cnt', 'data-type': 'int'}],\n",
       "  'datasets': ['bikeshare_processed']}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_id = 'bikeshare_forecasting'\n",
    "model_id = 'processed_features_mlp'\n",
    "\n",
    "# create a project to organize our models\n",
    "fiddler_api.create_project(project_id)\n",
    "\n",
    "# (re-)upload our model\n",
    "if model_id in fiddler_api.list_models(project_id):\n",
    "    fiddler_api.delete_model(project_id, model_id)\n",
    "fiddler_api.upload_model_custom(\n",
    "    artifact_path=model_dir, \n",
    "    info=model_of_processed_features_info, \n",
    "    project_id=project_id, \n",
    "    model_id=model_id,\n",
    "    associated_dataset_ids=['bikeshare_processed']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted_bike_rentals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32.117157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22.056610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.183211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   predicted_bike_rentals\n",
       "0               32.117157\n",
       "1               22.056610\n",
       "2               12.183211"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model executes properly!\n"
     ]
    }
   ],
   "source": [
    "# verify the model works\n",
    "pred = fiddler_api.run_model(project_id, model_id, df_2012_processed.head(3))\n",
    "success = np.isclose(pred, model.predict(x_test_processed[:3])).all()\n",
    "display(pred)\n",
    "assert success\n",
    "if success:\n",
    "    print('Model executes properly!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Putting it all together\n",
    "Whew, that felt like a lot building up a working `package.py`! Luckly, most of the above was just going into detail, the actual code needed to upload a TF model is not so long. Here we'll put everything needed into a single cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project already exists, no change.\n",
      "Running on Fiddler, the model predicts 10.65 for the first example row!\n"
     ]
    }
   ],
   "source": [
    "# preliminary info\n",
    "project_id = 'bikeshare_forecasting'\n",
    "model_id = 'processed_features_mlp'\n",
    "tf_model = model\n",
    "example_data = df_2011_processed\n",
    "target_column_name = 'cnt'\n",
    "model_dir = pathlib.Path('tf_model')\n",
    "\n",
    "# create a ModelInfo\n",
    "model_of_processed_features_info = fdl.ModelInfo.from_dataset_info(\n",
    "    dataset_info=fdl.DatasetInfo.from_dataframe(example_data),\n",
    "    target=target_column_name,\n",
    "    features=example_data.columns.difference([target_column_name]).tolist(),\n",
    "    input_type=fdl.ModelInputType.TABULAR,\n",
    ")\n",
    "\n",
    "# (re-)create directory for the model\n",
    "shutil.rmtree(model_dir, ignore_errors=True)\n",
    "model_dir.mkdir()\n",
    "\n",
    "# save the tensorflow model\n",
    "tf.saved_model.save(tf_model, str(model_dir / 'saved_model'))\n",
    "\n",
    "# create package.py\n",
    "package_py_contents = '''\n",
    "import pathlib\n",
    "\n",
    "import yaml\n",
    "\n",
    "import fiddler as fdl\n",
    "from fiddler.model_loaders import TFSavedModel\n",
    "\n",
    "MODEL_DIR = pathlib.Path(__file__).parent\n",
    "\n",
    "def load_model_info(model_dir):\n",
    "    \"\"\"Load ModelInfo from a model.yaml file\"\"\"\n",
    "    with (pathlib.Path(model_dir) / 'model.yaml').open('r') as yaml_file:\n",
    "        return fdl.ModelInfo.from_dict(yaml.load(yaml_file, Loader=yaml.SafeLoader))\n",
    "\n",
    "def get_model(model_dir=MODEL_DIR, tf_saved_model_dir=MODEL_DIR / 'saved_model'):\n",
    "    model_info = load_model_info(model_dir)\n",
    "    output_names = model_info.get_output_names()\n",
    "    is_binary_classification = (\n",
    "        model_info.model_task.name \n",
    "            == fdl.ModelTask.BINARY_CLASSIFICATION.name\n",
    "    )\n",
    "    return TFSavedModel(\n",
    "        tf_saved_model_dir, \n",
    "        output_column_names=output_names,\n",
    "        is_binary_classification=is_binary_classification\n",
    "    )\n",
    "'''\n",
    "with (model_dir / 'package.py').open('w') as f:\n",
    "    f.write(package_py_contents)\n",
    "    \n",
    "# upload the model\n",
    "\n",
    "\n",
    "# (re-)upload our model\n",
    "fiddler_api.create_project(project_id)\n",
    "if model_id in fiddler_api.list_models(project_id):\n",
    "    fiddler_api.delete_model(project_id, model_id)\n",
    "fiddler_api.upload_model_custom(\n",
    "    artifact_path=model_dir, \n",
    "    info=model_of_processed_features_info, \n",
    "    project_id=project_id, \n",
    "    model_id=model_id,\n",
    "    associated_dataset_ids=['bikeshare_processed']\n",
    ")\n",
    "\n",
    "# clean up local directory\n",
    "shutil.rmtree(model_dir, ignore_errors=True)\n",
    "\n",
    "# verify the uploaded model runs\n",
    "pred = fiddler_api.run_model(project_id, model_id, example_data.head(1))\n",
    "print(f'Running on Fiddler, the model predicts {pred.iat[0,0]:.2f} for the first example row!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Deploying feature preprocessing to Fiddler\n",
    "You probably noticed that the above example runs on the `bikeshare_processed` dataset. Below, we show how to upload the `preprocessor` object along with the TF model and configure Fiddler to run the preprocessor alongside the model. We will do this by serializing the `preprocessor` using `pickle` and passing a custom `input_transformation` function to the `TFSavedModel` in our `package.py` which tells it to un-pickle the preprocessor and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom input transformation works!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# (re-)create directory for the model\n",
    "shutil.rmtree(model_dir, ignore_errors=True)\n",
    "model_dir.mkdir()\n",
    "\n",
    "# save the preprocessor\n",
    "with (model_dir / 'preprocessor.pkl').open('wb') as pkl_file:\n",
    "    pickle.dump(preprocessor, pkl_file)\n",
    "\n",
    "# save the tensorflow model\n",
    "tf.saved_model.save(tf_model, str(model_dir / 'saved_model'))\n",
    "\n",
    "# define a custom input_transformation\n",
    "def custom_input_transformation(input_df):\n",
    "    # load preprocessor\n",
    "    with (model_dir / 'preprocessor.pkl').open('rb') as pkl_file:\n",
    "        preproc = pickle.load(pkl_file)\n",
    "    return [preproc.transform(input_df)]\n",
    "\n",
    "# verify this transformation works\n",
    "success = np.all(custom_input_transformation(x_train) == x_train_processed)\n",
    "assert success\n",
    "print('Custom input transformation works!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, let's also create a new ModelInfo\n",
    "model_info = fdl.ModelInfo.from_dataset_info(\n",
    "    dataset_info=fdl.DatasetInfo.from_dataframe(df_2011),\n",
    "    target=target_column_name,\n",
    "    features=example_data.columns.difference([target_column_name]).tolist(),\n",
    "    input_type=fdl.ModelInputType.TABULAR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All test-set predictions differ by less than 0.001.\n"
     ]
    }
   ],
   "source": [
    "# load a TFSavedModel with this custom input transformation\n",
    "fiddler_model = TFSavedModel(\n",
    "    saved_model_path=model_dir / 'saved_model', \n",
    "    output_column_names=model_info.get_output_names(),\n",
    "    input_transformation=custom_input_transformation\n",
    ")\n",
    "\n",
    "# verify this model works\n",
    "margin_of_error = 0.001\n",
    "max_error = np.max(np.abs(fiddler_model.predict(x_test) - model.predict(x_test_processed)))[0]\n",
    "success = max_error < margin_of_error\n",
    "assert success\n",
    "print(f'Success! All test-set predictions differ by less than {margin_of_error}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speeding things up\n",
    "There are a few tweaks we can make to ensure our model runs quickly on Fiddler. \n",
    "\n",
    "1. For models with a small memory footprint, we can safely increase the batch size of the TFSavedModel model loader, which will reduce the overhead incurred by feeding the Tensorflow graph many times.\n",
    "\n",
    "2. Since Fiddler caches models rather than always reloading from disk, we can create a custom_input_transformation that loads the preprocessor before the model is initialized, as opposed to afterward during every .predict() call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All test-set predictions differ by less than 0.001.\n"
     ]
    }
   ],
   "source": [
    "# define a custom input_transformation\n",
    "def get_fast_model():\n",
    "    # load preprocessor before we define custom_input_transformation()\n",
    "    with (model_dir / 'preprocessor.pkl').open('rb') as pkl_file:\n",
    "        preproc = pickle.load(pkl_file)\n",
    "\n",
    "    def custom_input_transformation(input_df):\n",
    "        return [preproc.transform(input_df)]\n",
    "    \n",
    "    return TFSavedModel(\n",
    "        saved_model_path=model_dir / 'saved_model', \n",
    "        output_column_names=model_info.get_output_names(),\n",
    "        # use a big batch size\n",
    "        batch_size=512,\n",
    "        input_transformation=custom_input_transformation,\n",
    "    )\n",
    "\n",
    "fiddler_model = get_fast_model()\n",
    "max_error = np.max(np.abs(fiddler_model.predict(x_test) - model.predict(x_test_processed)))[0]\n",
    "success = max_error < margin_of_error\n",
    "assert success\n",
    "print(f'Success! All test-set predictions differ by less than {margin_of_error}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together with feature preprocessing\n",
    "Let's put this all together to show how we can upload our model along with the preprocessor!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project already exists, no change.\n",
      "Running on Fiddler, the model predicts 32.12 for the first example row!\n"
     ]
    }
   ],
   "source": [
    "# preliminary info\n",
    "project_id = 'bikeshare_forecasting'\n",
    "model_id = 'raw_features_mlp'\n",
    "tf_model = model\n",
    "example_data = pd.concat([x_test, y_test], axis=1)\n",
    "target_column_name = 'cnt'\n",
    "model_dir = pathlib.Path('tf_model')\n",
    "\n",
    "# create a ModelInfo\n",
    "model_info = fdl.ModelInfo.from_dataset_info(\n",
    "    dataset_info=fdl.DatasetInfo.from_dataframe(example_data),\n",
    "    target=target_column_name,\n",
    "    features=example_data.columns.difference([target_column_name]).tolist(),\n",
    "    input_type=fdl.ModelInputType.TABULAR,\n",
    ")\n",
    "\n",
    "# (re-)create directory for the model\n",
    "shutil.rmtree(model_dir, ignore_errors=True)\n",
    "model_dir.mkdir()\n",
    "\n",
    "# save the preprocessor\n",
    "with (model_dir / 'preprocessor.pkl').open('wb') as pkl_file:\n",
    "    pickle.dump(preprocessor, pkl_file)\n",
    "\n",
    "# save the tensorflow model\n",
    "tf.saved_model.save(tf_model, str(model_dir / 'saved_model'))\n",
    "\n",
    "# create package.py\n",
    "package_py_contents = '''\n",
    "import pathlib\n",
    "import pickle\n",
    "\n",
    "import yaml\n",
    "\n",
    "import fiddler as fdl\n",
    "from fiddler.model_loaders import TFSavedModel\n",
    "\n",
    "MODEL_DIR = pathlib.Path(__file__).parent\n",
    "\n",
    "def load_model_info(model_dir):\n",
    "    \"\"\"Load ModelInfo from a model.yaml file\"\"\"\n",
    "    with (pathlib.Path(model_dir) / 'model.yaml').open('r') as yaml_file:\n",
    "        return fdl.ModelInfo.from_dict(yaml.load(yaml_file, Loader=yaml.SafeLoader))\n",
    "\n",
    "def get_model(model_dir=MODEL_DIR, tf_saved_model_dir=MODEL_DIR / 'saved_model'):\n",
    "    model_info = load_model_info(model_dir)\n",
    "    output_names = model_info.get_output_names()\n",
    "    is_binary_classification = (\n",
    "        model_info.model_task.name \n",
    "            == fdl.ModelTask.BINARY_CLASSIFICATION.name\n",
    "    )\n",
    "    # load preprocessor before we define custom_input_transformation()\n",
    "    with (MODEL_DIR / 'preprocessor.pkl').open('rb') as pkl_file:\n",
    "        preproc = pickle.load(pkl_file)\n",
    "\n",
    "    def custom_input_transformation(input_df):\n",
    "        return [preproc.transform(input_df)]\n",
    "    \n",
    "    return TFSavedModel(\n",
    "        tf_saved_model_dir, \n",
    "        output_column_names=output_names,\n",
    "        is_binary_classification=is_binary_classification,\n",
    "        # use a big batch size since the model is small\n",
    "        batch_size=512,\n",
    "        input_transformation=custom_input_transformation,\n",
    "    )\n",
    "'''\n",
    "with (model_dir / 'package.py').open('w') as f:\n",
    "    f.write(package_py_contents)\n",
    "\n",
    "# (re-)upload our model\n",
    "fiddler_api.create_project(project_id)\n",
    "if model_id in fiddler_api.list_models(project_id):\n",
    "    fiddler_api.delete_model(project_id, model_id)\n",
    "fiddler_api.upload_model_custom(\n",
    "    artifact_path=model_dir, \n",
    "    info=model_info, \n",
    "    project_id=project_id, \n",
    "    model_id=model_id,\n",
    "    associated_dataset_ids=['bikeshare']\n",
    ")\n",
    "\n",
    "# clean up local directory\n",
    "shutil.rmtree(model_dir, ignore_errors=True)\n",
    "\n",
    "# verify the uploaded model runs\n",
    "pred = fiddler_api.run_model(project_id, model_id, example_data.head(1))\n",
    "print(f'Running on Fiddler, the model predicts {pred.iat[0,0]:.2f} for the first example row!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
