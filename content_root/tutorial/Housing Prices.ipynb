{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Housing Prices Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Fiddler Client\n",
    "We begin this section as usual by establishing a connection to our\n",
    "Fiddler instance. We can establish this connection either by specifying \n",
    "our credentials directly, or by utilizing our `fiddler.ini` file. More\n",
    "information can be found in the [setup](https://github.com/fiddler-labs/fiddler-samples/blob/master/content_root/tutorial/00%20Setup.ipynb) section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiddler as fdl\n",
    "\n",
    "# client = fdl.FiddlerApi(url=url, org_id=org_id, auth_token=auth_token)\n",
    "client = fdl.FiddlerApi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load baseline\n",
    "Here we will load in our baseline dataset from a csv called `housing-prices.csv`. We will\n",
    "also create a schema using this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "baseline_df = pd.read_csv('/app/fiddler_samples/samples/datasets/housing/housing-prices.csv')\n",
    "baseline_schema = fdl.DatasetInfo.from_dataframe(baseline_df, max_inferred_cardinality=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Monitoring\n",
    "Now, we will setup a project, and use Fiddler's surrogate model capability to generate a model. \n",
    "Projects are one of the key entities of Fiddler. They are convenient containers \n",
    "for housing the models and datasets associated with a given ML use case. Specifics about both\n",
    "projects and models can be found [here](https://docs.fiddler.ai/components/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = 'tutorial'\n",
    "name = 'housing_prices'\n",
    "target='SalePrice'\n",
    "input_df = baseline_df.drop(columns=['Id', target])\n",
    "features = list(input_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## setup/cleanup project\n",
    "if project_id in client.list_projects():\n",
    "    client.delete_model(project_id, name)\n",
    "    client.delete_dataset(name)\n",
    "else:\n",
    "    client.create_project(project_id)\n",
    "\n",
    "\n",
    "client.create_surrogate_model(\n",
    "    project_id,\n",
    "    name,\n",
    "    baseline_df,\n",
    "    baseline_schema,\n",
    "    target,\n",
    "    features\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send Monitoring Events\n",
    "In this step, we will be simulating traffic to send for our model monitoring by using \n",
    "[publish_event](https://docs.fiddler.ai/api-reference/python-package/#publish-event). \n",
    "This will be the equivalent of running our model separately on data, and either \n",
    "sending to Fiddler then, or saving this information to a log and sending at a later point.\n",
    "\n",
    "For the purposes of this demonstration, we will be going with a log approach simulated\n",
    "by using our original dataframe.  To most accurately simulate this as a time-series event,\n",
    "we will also be calling  a function to generate a timestamp in the last two weeks. Real \n",
    "data will ideally  have a timestamp related to when the event took place; otherwise, \n",
    "the current time will be used.\n",
    "\n",
    "**Note**: The timestamp must be in UTC milliseconds. See \n",
    "[here](https://docs.fiddler.ai/api-reference/python-package/#publish-event) for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from random import sample, randint\n",
    "NUM_EVENTS_TO_SEND = 50\n",
    "\n",
    "def getTimestampFromPastTwoWeeks():\n",
    "    \"\"\"\n",
    "    Generate a randomized timestamp from the past two weeks. Timestamp is in \n",
    "    milliseconds since epoch in UTC.\n",
    "    \"\"\"\n",
    "    TWO_WEEKS_MS = 604800 * 2 * 1000\n",
    "    current_time_in_ms = round(time.time() * 1000)\n",
    "    \n",
    "    random_time_in_past_two_weeks = current_time_in_ms - randint(0, TWO_WEEKS_MS)\n",
    "    return random_time_in_past_two_weeks\n",
    "\n",
    "# Convert this dataframe into a list of dictionary events, where each event is its own dictionary\n",
    "event_list_dict = input_df.sample(n=NUM_EVENTS_TO_SEND, random_state=42).to_dict(orient='records') \n",
    "\n",
    "for ind, event_dict in enumerate(event_list_dict):\n",
    "    event_ms_time_stamp = getTimestampFromPastTwoWeeks()\n",
    "    result = client.publish_event(project_id, model_id, event_dict, event_time_stamp=event_ms_time_stamp)\n",
    "    \n",
    "    clear_output(wait = True)\n",
    "    readable_timestamp = datetime.datetime.fromtimestamp(event_ms_time_stamp/1000.0)\n",
    "    \n",
    "    print(f'Sending {ind+1} / {NUM_EVENTS_TO_SEND} \\n{readable_timestamp} UTC: \\n{event_dict}')\n",
    "    time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
